<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<link rel="shortcut icon" href="myIcon.ico">
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
    
<meta name="keywords" content="Qianyu Zhou, Shanghai Jiao Tong University"> 
<meta name="description" content="Qianyu Zhou's home page">
<meta name="google-site-verification" content="google1367f398cef8d4d4.html" />
<meta name="google-site-verification" content="google1367f398cef8d4d4.html" />
<link rel="stylesheet" href="jemdoc.css" type="text/css">
<title> Qianyu Zhou's homepage</title>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-87320911-1', 'auto');
  ga('send', 'pageview');
</script>

<script>
   function showPubs(id) {
  if (id == 0) {
    document.getElementById('pubs').innerHTML = document.getElementById('pubs_selected').innerHTML;
    document.getElementById('select0').style = 'text-decoration:underline;color:#000000';
    document.getElementById('select1').style = '';
  } else {
    document.getElementById('pubs').innerHTML = document.getElementById('pubs_by_topic').innerHTML;
    document.getElementById('select1').style = 'text-decoration:underline;color:#000000';
    document.getElementById('select0').style = '';
  }
}

</script>
</head>
<body>

<nav class="navbar navbar-dark navbar-expand-lg fixed-top">
    <div id="layout-menu">
        <a href="#biography">Biography</a>
        <a href="#interests">Reserach Interests</a>
        <a href="#news">News</a>
        <a href="#pubs">Publication</a>
        <a href="#education">Education</a>
        <a href="#experience">Experience</a>
        <a href="#service">Services</a>
        <a href="#honors">Honors</a>
        <!-- <a href="#students">Students</a> -->
<!--         <a href="chinese_homepage.html">Chinese Homepage</a>
 -->    </div>
</nav>

<div id="layout-content" style="margin-top:25px">
<table>
    <tbody>
        <tr>
            <td width="670">
                <div id="toptitle">                 
                    <h1><font face="Arial"> Qianyu Zhou </font></h1>
                </div>

                <!-- <h3><font face="Arial"> Specially-Appointed Assistant Professor </font></h3>
                <p><font face="Arial"> 
                    <a href="https://www.mi.t.u-tokyo.ac.jp/en" target="_blank"> Harada Lab. (Machine Intelligence Lab) </a>  <br>
                    <a href="https://www.rcast.u-tokyo.ac.jp/en/about/index.html" target="_blank"> Research Center for Advanced Science and Technology</a> <br>
                    <a href="https://www.u-tokyo.ac.jp/en/" target="_blank"> The University of Tokyo</a> <br>
                    Tokyo, Japan <br>
                    <br>
                    Email: <a href="mailto:q-zhou@mi.t.u-tokyo.ac.jp">q-zhou@mi.t.u-tokyo.ac.jp</a> 
                    <span style="margin-right: 20px;"></span> 
                    <a href="https://scholar.google.com/citations?hl=en&pli=1&user=KHg04fkAAAAJ">Google Scholar</a> 
                    <span style="margin-right: 20px;"></span> 
                    <a href="https://dblp.uni-trier.de/pid/232/4830-1.html">DBLP</a>
                    <span style="margin-right: 20px;"></span>
                    <a href="https://www.semanticscholar.org/author/Qianyu-Zhou/67190665">Semantic Scholar</a> 
                    <span style="margin-right: 20px;"></span>
                    <a href="https://orcid.org/0000-0002-5331-050X">ORCID</a> 
                    <br>
                </font></p> -->
            </td>
            <td>
                <img src="qianyu_1cun.jpg" border="0" width="160"><br>
            </td>
        </tr><tr>
    </tr></tbody>
</table>
 
<div id="biography">
<h2><font face="Arial"> Biography </font></h2>
<!-- <a name="Biography" /> -->
<p style="text-align:justify";><font face="Arial">
I got the Ph.D. degree at the <a href="https://www.cs.sjtu.edu.cn/en/" target="_blank">
Department of Computer Science and Engineering</a>, as well as <a href="https://dmcv.sjtu.edu.cn/" target="_blank">DMCV Lab</a>, <a href="https://en.sjtu.edu.cn/" target="_blank">Shanghai Jiao Tong University (SJTU)</a>, supervised by 
Prof. <a href="https://dmcv.sjtu.edu.cn/people/" target="_blank">
Lizhuang Ma</a>. I was selected into <a href="https://xsb.seiee.sjtu.edu.cn/xsb/info/35105.htm" target="_blank">
Wenjun Wu AI Honored Ph.D Class (Top 1%)</a> and <a href="https://www.gs.sjtu.edu.cn/post/detail/Z3MxNDc2" target="_blank">
Zhiyuan Honored Ph.D Program (The Highest Honor)</a> of SJTU, co-advised by Dr. <a href="https://shijianping.me/" target="_blank">
Jianping Shi</a>.
Before that, I received the B.Sc degree from the <a href="http://ccst.jlu.edu.cn/" target="_blank">College of Computer Science and Technology</a>, <a href="http://global.jlu.edu.cn/" target="_blank">Jilin University</a> in 2019. 
I have published 30+ papers at top-tier journals and conferences (1500+ citations) such as IEEE TPAMI, CVPR, ICCV, ECCV, NeurIPS, ACM MM, AAAI, and etc. My research has won the 2024 World Internet Conference (WIC) award for Pioneering Science and Technology, the Top 3% Paper Recognition Award of ICASSP 2023, and Best Paper Award of CAD/Graphics 2023. 
<!-- Here is my <a href="Qianyu_Zhou_CV.pdf">Curriculum Vitae</a>. -->

<!-- Currently, I am interested in computer vision, transfer learning and vision-language models, especially domain adaptation, domain generalization for image classification, object detection and semantic segmentation. -->

<!-- </font></p> 
<div class="navbar-collapse collapse">
<h2>
<a href="#Biography">Biography|</a>
<a href="#News">News|</a>
<a href="#Publications">Publications|</a>
<a href="#Services">Services|</a>  
<a href="#Experience">Experience</a>   
</h2>
</div>  -->

<!-- <p style="text-align:justify"><font face="Arial">
My research interest lies at the intersection of computer vision and machine learning.
Currently, I am working on (i) open-world perception and adaptation, (ii) out-of-domain generalization, and (iii) graph reasoning for vision tasks.
</font></p> -->
    
<!--<font color="red">Currently, I am working on domain generalization for semantic segmentation and object detection. I am looking for undergraduate or master students to engage in ongoing research papers. Don't hesitate to email me if you are interested.</font> <br><br>-->



<h2><font face="Arial"> Reserach Interests </h2>
<div id="interests">
<p><font face="Arial">Computer Vision, Computer Graphics, Machine Intelligence. </font></p> 
<ul>
    <li> <p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;">    
        Transfer Learning, e.g., domain adaptation, domain generalization.  </p></li> 
<!--     <li> <p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;">    
        Scene Understanding, e.g., semantic segmentation, object detection in images and videos.  </p></li>  -->
    <li> <p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;">    
        3D Vision, e.g., point cloud completion, point cloud understanding.  </p></li>  
    <li> <p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;">    
        Embodied AI, e.g., sim-to-real adaptation, embodied world model.  </p></li>
    <li> <p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;">    
        Biometrics, e.g., face anti-spoofing, adversarial attacks.  </p></li>  
</ul>
</div>


<h2><font face="Arial"> News </h2>
<div id="news" style="height: 330px; overflow: auto;">
<!-- <a name="News" /> -->
<ul>
    <li> <p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;">    
        [06/2025] One paper is accepted to ICCV 2025 (24% acceptance rate). </p></li>
    <li> <p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;">    
        [06/2025] One paper is accepted to TIP (24% acceptance rate, IF=10.8). </p></li>
    <li> <p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;">    
        [04/2025] One paper is accepted to IJCAI 2025 (19.3% acceptance rate). </p></li>
    <li> <p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;">    
        [03/2025] Three papers are accepted to ICME 2025 (30% acceptance rate). </p></li>
    <li> <p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;">    
        [02/2025] One paper is accepted to CVPR 2025 (22.1% acceptance rate). </p></li>
    <li> <p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;">    
        [12/2024] Three papers are accepted to AAAI 2025 (23.4% acceptance rate). </p></li>
    <li> <p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;">    
        [12/2024] One paper is accepted to ICASSP 2025 (49% acceptance rate). </p></li>
    <li> <p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;">    
        [11/2024] I got the 2024 World Internet Conference (WIC) award for Pioneering Science and Technology. </p></li>
    <li> <p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;">    
        [10/2024] I am invited to be an oral session chair of ACM MM 2024. </p></li>
    <li> <p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;">    
        [09/2024] Two papers are accepted to NeurIPS 2024 (25.80% acceptance rate). </p></li>
    <li> <p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;">    
        [07/2024] Three papers are accepted to ACM MM 2024 (26.20% acceptance rate). </p></li>
    <li> <p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;">    
        [07/2024] Two papers are accepted to ECCV 2024 (27.90% acceptance rate). </p></li>
    <li> <p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;">    
        [06/2024] I got my Ph.D. degree in Department of CSE, SJTU. </p></li>
    <li> <p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;">    
        [06/2024] One paper is accepted to TCSVT (15% acceptance rate, IF=8.4). </p></li>
    <li> <p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;">    
        [05/2024] I have successfully defended my PhD thesis!. </p></li>
    <li> <p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;">    
        [03/2024] One paper is accepted to TVCG (25% acceptance rate, IF=5.6). </p></li>
    <li> <p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;">    
        [02/2024] Two papers are accepted to CVPR 2024 (23.58% acceptance rate). </p></li>
    <li> <p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;">    
        [01/2024] Our paper TransVOD (TPAMI 2023) becomes the ESI Highly Cited Paper (Top 1%). </p></li>
    <li> <p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;">    
        [12/2023] One paper is accepted to AAAI 2024 (23.75% acceptance rate). </p></li>
    <li> <p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;">    
        [09/2023] I won the Yuanqing Yang Ph.D Fellowship (3 recipients per year in SJTU).  </p></li> 
    <li> <p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;">    
        [08/2023] One paper won the Best Paper Award in CAD/Graphics 2023.  </p></li> 
    <li> <p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;">    
        [06/2023] One paper is selected into Top 3% Paper Recognition of all papers accepted at ICASSP 2023.  </p></li> 
    <li> <p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;">    
        [05/2023] One paper is accepted to the 1st Workshop on Vision-based InduStrial InspectiON (VISION) of CVPR 2023.  </p></li>  
<!--     <li> <p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;">    
        [04/2023] I am invited to serve as a reviewer for ACM MM 2023. </p></li>   -->
    <li> <p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;">    
        [03/2023] One paper is accepted to CVPR 2023 (25.78% acceptance rate). </p></li>
<!--     <li> <p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;">    
        [02/2023] I am invited to serve as a reviewer for ICCV 2023. </p></li>   -->  
    <li> <p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;">    
        [01/2023] One paper is accepted to TPAMI (9% acceptance rate, IF=23.6). </p></li>
    <li> <p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;">    
        [11/2022] One paper is accepted to TPAMI (9% acceptance rate, IF=23.6). </p></li>   
<!--     <li> <p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;">    
        [11/2022] I am invited to serve as a reviewer for CVPR 2023. </p></li> -->
    <li> <p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;">    
        [09/2022] One paper is accepted to TCSVT (15% acceptance rate, IF=8.4). </p></li>
    <li> <p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;">    
        [08/2022] I am invited to serve as a Programme Committee member of AAAI 2023. </p></li>
    <li> <p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;">    
        [07/2022] One paper is accepted to ECCV 2022 (26% acceptance rate). </p></li>
    <li> <p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;">    
        [06/2022] One paper is accepted to ACM MM 2022 (27.9% acceptance rate). </p></li>
    <li> <p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;">    
        [05/2022] One paper is accepted to Pattern Recognition (19% acceptance rate, IF=8.0). </p></li>
    <li> <p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;">    
        [05/2022] One paper is accepted to CVIU (17% acceptance rate, IF=4.5). </p></li>
    <li> <p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;">    
        [03/2022] One paper is selected as an oral presentation of ICME 2022 (top 14.5%). </p></li>
<!--     <li> <p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;">    
        [03/2022] I am invited to serve as a reviewer for ECCV 2022. </p></li> -->
</ul>
</div>

<!-- <h2><font face="Arial"> Preprints </font></h2>
<div id="preprint">
<ul>

    <div class="paper" style="clear:left;">
      <div class="pimg" style="float:left;margin-bottom:10px;padding-left:3px;border-bottom:solid 1px #EEE;box-shadow:0 0px 2px"><font face="Arial" size="3">
          <img src="imgs/fig_mmvm_arxiv_25.png" width="200" >
      </div>
      <div class="ptitle" style="padding:1px;margin-left:215px;">
          Are They the Same? Exploring Visual Correspondence Shortcomings of Multimodal LLMs
      </div>
      <div class="pauthors" style="padding:1px;margin-left:215px;">
          Yikang Zhou, Tao Zhang, Shilin Xu, Shihao Chen, <b>Qianyu Zhou</b>, Yunhai Tong, Shunping Ji, Jiangning Zhang, Xiangtai Li, Lu Qi. 
      </div>
      <div class="pvenue" style="padding:1px;margin-left:215px;">
          ArXiv preprint arXiv:2501.04670
          <p>
            [<a href="https://arxiv.org/abs/2501.04670">paper</a>]
            [<a href="https://arxiv.org/abs/2501.04670">arxiv</a>]
            [<a href="https://github.com/zhouyiks/CoLVA">code</a>]
            [<a href="bibs/Arxiv_2025_MMVM.txt">bib</a>]
          </p>
      </div>
    </div>


    <div class="paper" style="clear:left;">
      <div class="pimg" style="float:left;margin-bottom:10px;padding-left:3px;border-bottom:solid 1px #EEE;box-shadow:0 0px 2px"><font face="Arial" size="3">
          <img src="imgs/fig_dip_arxiv_24.png" width="200" >
      </div>
      <div class="ptitle" style="padding:1px;margin-left:215px;">
          Diffusion Implicit Policy for Unpaired Scene-aware Motion Synthesis
      </div>
      <div class="pauthors" style="padding:1px;margin-left:215px;">
          Jingyu Gong, Chong Zhang, Fengqi Liu, Ke Fan, <b>Qianyu Zhou</b>, Xin Tan, Zhizhong Zhang, Yuan Xie, Lizhuang Ma.
      </div>
      <div class="pvenue" style="padding:1px;margin-left:215px;">
          ArXiv preprint arXiv:2412.02261.
          <p>
            [<a href="https://arxiv.org/pdf/2412.02261">paper</a>]
            [<a href="https://arxiv.org/pdf/2412.02261">arxiv</a>]
            [<a href="https://jingyugong.github.io/DiffusionImplicitPolicy/">code</a>]
            [<a href="bibs/Arxiv_2024_DIP.txt">bib</a>]
          </p>
      </div>
    </div>

</ul>
</div> -->

<p id="publications">
<h2><font face="Arial"> Publications
    (<a href="" id="select0" onclick="showPubs(0); return false;">show selected</a> /
     <a href="" id="select1" onclick="showPubs(1); return false;">show all</a>)
</h2>
</p>
<!-- <div id="publication">
 -->


<p><font face="Arial"><a href="https://scholar.google.com/citations?hl=en&pli=1&user=KHg04fkAAAAJ">[Google Scholar]</a> (* indicates joint first author, and † indicates corresponding author) </font></p> 

<!-- Summary(18): TPAMI x2, CVPR x3, ICCV x1, ECCV x2, ACM MM x4, AAAI x1, TCSVT/PR x3, others x2
 -->
<div id="pubs"></div>
<script id="pubs_selected"> 
Summary(22): TPAMI(2)+TIP(1), CVPR/ICCV/ECCV(3/1/2), NeurIPS(1), ACM MM(4), AAAI(3), TCSVT/PR(3), others(2);</p>
<ul>
    <div class="paper" style="clear:left;">
      <div class="pimg" style="float:left;margin-bottom:10px;padding-left:3px;border-bottom:solid 1px #EEE;box-shadow:0 0px 2px"><font face="Arial" size="3">
          <img src="imgs/fig_transvod_tpami_23.png" width="200" >
      </div>
      <div class="ptitle" style="padding:1px;margin-left:215px;">
          TransVOD: End-to-End Video Object Detection with Spatial-Temporal Transformers
      </div>
      <div class="pauthors" style="padding:1px;margin-left:215px;">
          <b>Qianyu Zhou</b>, Xiangtai Li, Lu He, Yibo Yang, Guangliang Cheng, Yunhai Tong, Lizhuang Ma, Dacheng Tao. 
      </div>
      <div class="pvenue" style="padding:1px;margin-left:215px;">
          IEEE Transactions on Pattern Analysis and Machine Intelligence (<b>TPAMI</b>), 2023.
          <p>
           [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9960850">paper</a>]
           [<a href="https://arxiv.org/pdf/2201.05047.pdf">arxiv</a>] 
           [<a href="https://ieeexplore.ieee.org/document/9960850">DOI</a>]
           [<a href="https://github.com/SJTU-LuHe/TransVOD">code1</a>]
           [<a href="https://github.com/qianyuzqy/TransVOD_Lite">code2</a>]
           [<a href="https://github.com/qianyuzqy/TransVOD_plusplus">code3</a>]
           [<a href="bibs/TPAMI_2023_TransVOD.txt">bib</a>]
          </p>
      </div>
    </div>


    <div class="paper" style="clear:left;">
      <div class="pimg" style="float:left;margin-bottom:10px;padding-left:3px;border-bottom:solid 1px #EEE;box-shadow:0 0px 2px"><font face="Arial" size="3">
          <img src="imgs/fig_sad_tpami_23.png" width="200" >
      </div>
      <div class="ptitle" style="padding:1px;margin-left:215px;">
          Self-Adversarial Disentangling for Specific Domain Adaptation
      </div>
      <div class="pauthors" style="padding:1px;margin-left:215px;">
          <b>Qianyu Zhou</b>, Qiqi Gu, Jiangmiao Pang,  Xuequan Lu, Lizhuang Ma. 
      </div>
      <div class="pvenue" style="padding:1px;margin-left:215px;">
          IEEE Transactions on Pattern Analysis and Machine Intelligence (<b>TPAMI</b>), 2023.
          <p>
           [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10024368">paper</a>]
           [<a href="https://arxiv.org/pdf/2108.03553.pdf">arxiv</a>] 
           [<a href="https://ieeexplore.ieee.org/document/10024368">DOI</a>]
           [<a href="bibs/TPAMI_2023_SAD.txt">bib</a>]
          </p>
      </div>
    </div>

    <div class="paper" style="clear:left;">
      <div class="pimg" style="float:left;margin-bottom:10px;padding-left:3px;border-bottom:solid 1px #EEE;box-shadow:0 0px 2px"><font face="Arial" size="3">
          <img src="imgs/fig_iadg_cvpr_23.png" width="200" >
      </div>
      <div class="ptitle" style="padding:1px;margin-left:215px;">
          Instance-Aware Domain Generalization for Face Anti-Spoofing
      </div>
      <div class="pauthors" style="padding:1px;margin-left:215px;">
          <b>Qianyu Zhou*</b>, Ke-Yue Zhang*, Taiping Yao, Xuequan Lu, Ran Yi,  Shouhong Ding, Lizhuang Ma. 
      </div>
      <div class="pvenue" style="padding:1px;margin-left:215px;">
          IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2023.
          <p>
             [<a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_Instance-Aware_Domain_Generalization_for_Face_Anti-Spoofing_CVPR_2023_paper.pdf">paper</a>]
             [<a href="https://arxiv.org/pdf/2304.05640.pdf">arxiv</a>] 
             [<a href="https://doi.org/10.1109/CVPR52729.2023.01959">DOI</a>]
             [<a href="https://github.com/qianyuzqy/IADG">code</a>]
             [<a href="bibs/CVPR_2023_IADG.txt">bib</a>]
          </p>
      </div>
    </div>

    <div class="paper" style="clear:left;">
      <div class="pimg" style="float:left;margin-bottom:10px;padding-left:3px;border-bottom:solid 1px #EEE;box-shadow:0 0px 2px"><font face="Arial" size="3">
          <img src="imgs/fig_ttdg_cvpr_24.png" width="200" >
      </div>
      <div class="ptitle" style="padding:1px;margin-left:215px;">
          Test-Time Domain Generalization for Face Anti-Spoofing
      </div>
      <div class="pauthors" style="padding:1px;margin-left:215px;">
          <b>Qianyu Zhou*</b>, Ke-Yue Zhang*, Taiping Yao, Xuequan Lu, Shouhong Ding, Lizhuang Ma. 
      </div>
      <div class="pvenue" style="padding:1px;margin-left:215px;">
          IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2024.
          <p>
             [<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhou_Test-Time_Domain_Generalization_for_Face_Anti-Spoofing_CVPR_2024_paper.pdf">paper</a>]
             [<a href="https://arxiv.org/pdf/2403.19334.pdf">arxiv</a>]
             [<a href="https://doi.org/10.1109/CVPR52733.2024.00025">DOI</a>]
             [<a href="bibs/CVPR_2024_TTDG.txt">bib</a>] 
          </p>
      </div>
    </div>

    <div class="paper" style="clear:left;">
      <div class="pimg" style="float:left;margin-bottom:10px;padding-left:3px;border-bottom:solid 1px #EEE;box-shadow:0 0px 2px"><font face="Arial" size="3">
          <img src="imgs/fig_camix_tcsvt_23.png" width="200" >
      </div>
      <div class="ptitle" style="padding:1px;margin-left:215px;">
          Context-Aware Mixup for Domain Adaptive Semantic Segmentation
      </div>
      <div class="pauthors" style="padding:1px;margin-left:215px;">
          <b>Qianyu Zhou</b>, Zhengyang Feng, Qiqi Gu, Jiangmiao Pang, Guangliang Cheng, Xuequan Lu, Jianping Shi, Lizhuang Ma. 
      </div>
      <div class="pvenue" style="padding:1px;margin-left:215px;">
          IEEE Transactions on Circuits and Systems for Video Technology (<b>TCSVT</b>), 2023.
          <p>
             [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9889681">paper</a>]
             [<a href="https://arxiv.org/pdf/2108.03557.pdf">arxiv</a>] 
             [<a href="https://ieeexplore.ieee.org/document/9889681">DOI</a>]
             [<a href="https://github.com/qianyuzqy/CAMix">code</a>]
             [<a href="bibs/TCSVT_2023_CAMix.txt">bib</a>]
          </p>
      </div>
    </div>


    <div class="paper" style="clear:left;">
      <div class="pimg" style="float:left;margin-bottom:10px;padding-left:3px;border-bottom:solid 1px #EEE;box-shadow:0 0px 2px"><font face="Arial" size="3">
          <img src="imgs/fig_amel_mm_22.png" width="200" >
      </div>
      <div class="ptitle" style="padding:1px;margin-left:215px;">
          Adaptive Mixture of Experts Learning for Generalizable Face Anti-Spoofing
      </div>
      <div class="pauthors" style="padding:1px;margin-left:215px;">
          <b>Qianyu Zhou*</b>, Ke-Yue Zhang*, Taiping Yao*, Ran Yi,  Shouhong Ding, Lizhuang Ma. 
      </div>
      <div class="pvenue" style="padding:1px;margin-left:215px;">
          The 30th ACM International Conference on Multimedia (<b>ACM MM</b>), 2022.
          <p>
             [<a href="https://dl.acm.org/doi/pdf/10.1145/3503161.3547769">paper</a>]
             [<a href="https://arxiv.org/pdf/2207.09868.pdf">arxiv</a>]
             [<a href="https://dl.acm.org/doi/10.1145/3503161.3547769">DOI</a>]
             [<a href="bibs/MM_2022_AMEL.txt">bib</a>]
          </p>
      </div>
    </div>

    <div class="paper" style="clear:left;">
      <div class="pimg" style="float:left;margin-bottom:10px;padding-left:3px;border-bottom:solid 1px #EEE;box-shadow:0 0px 2px"><font face="Arial" size="3">
          <img src="imgs/fig_gda_eccv_22.png" width="200" >
      </div>
      <div class="ptitle" style="padding:1px;margin-left:215px;">
          Generative Domain Adaptation for Face Anti-Spoofing
      </div>
      <div class="pauthors" style="padding:1px;margin-left:215px;">
          <b>Qianyu Zhou*</b>, Ke-Yue Zhang*, Taiping Yao, Ran Yi,  Shouhong Ding, Lizhuang Ma. 
      </div>
      <div class="pvenue" style="padding:1px;margin-left:215px;">
          European Conference on Computer Vision (<b>ECCV</b>), 2022.
          <p>
             [<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136650328.pdf">paper</a>]
           [<a href="https://arxiv.org/pdf/2207.10015.pdf">arxiv</a>] 
           [<a href="https://link.springer.com/chapter/10.1007/978-3-031-20065-6_20">DOI</a>]
           [<a href="bibs/ECCV_2022_GDA.txt">bib</a>]
          </p>
      </div>
    </div>

    <div class="paper" style="clear:left;">
      <div class="pimg" style="float:left;margin-bottom:10px;padding-left:3px;border-bottom:solid 1px #EEE;box-shadow:0 0px 2px"><font face="Arial" size="3">
          <img src="imgs/fig_uacr_cviu_22.png" width="200" >
      </div>
      <div class="ptitle" style="padding:1px;margin-left:215px;">
          Uncertainty-Aware Consistency Regularization for Cross-Domain Semantic Segmentation
      </div>
      <div class="pauthors" style="padding:1px;margin-left:215px;">
          <b>Qianyu Zhou</b>, Zhengyang Feng, Qiqi Gu, Guangliang  Cheng, Xuequan Lu, Jianping Shi, Lizhuang Ma. 
      </div>
      <div class="pvenue" style="padding:1px;margin-left:215px;">
          Computer Vision and Image Understanding (<b>CVIU</b>), 2022.
          <p>
           [<a href="https://www.sciencedirect.com/science/article/pii/S1077314222000625">paper</a>]
           [<a href="https://arxiv.org/pdf/2004.08878.pdf">arxiv</a>]
           [<a href="https://www.sciencedirect.com/science/article/pii/S1077314222000625">DOI</a>]
           [<a href="bibs/CVIU_2022_UACR.txt">bib</a>] 
          </p>
      </div>
    </div>

    <div class="paper" style="clear:left;">
      <div class="pimg" style="float:left;margin-bottom:10px;padding-left:3px;border-bottom:solid 1px #EEE;box-shadow:0 0px 2px"><font face="Arial" size="3">
          <img src="imgs/fig_rccr_icme_22.png" width="200" >
      </div>
      <div class="ptitle" style="padding:1px;margin-left:215px;">
          Domain Adaptive Semantic Segmentation via Regional Contrastive Consistency Regularization
      </div>
      <div class="pauthors" style="padding:1px;margin-left:215px;">
          <b>Qianyu Zhou</b>, Chuyun Zhuang,  Ran Yi, Xuequan Lu, Lizhuang Ma. 
      </div>
      <div class="pvenue" style="padding:1px;margin-left:215px;">
          IEEE International Conference on Multimedia and Expo (<b>ICME</b>), 2022 (Oral Presentation).
          <p>
           [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9859793">paper</a>]
           [<a href="https://arxiv.org/pdf/2110.05170">arxiv</a>]
           [<a href="https://ieeexplore.ieee.org/document/9859793">DOI</a>]
           [<a href="https://github.com/qianyuzqy/RCCR">code</a>]
           [<a href="bibs/ICME_2022_RCCR.txt">bib</a>] 
          </p>
      </div>
    </div>

    <div class="paper" style="clear:left;">
      <div class="pimg" style="float:left;margin-bottom:10px;padding-left:3px;border-bottom:solid 1px #EEE;box-shadow:0 0px 2px"><font face="Arial" size="3">
          <img src="imgs/fig_dtcs_tip_25.png" width="200" >
      </div>
      <div class="ptitle" style="padding:1px;margin-left:215px;">
          Diverse Target and Contribution Scheduling for Domain Generalization
      </div>
      <div class="pauthors" style="padding:1px;margin-left:215px;">
          Shaocong Long*, <b>Qianyu Zhou*</b>, Chenhao Ying, Lizhuang Ma, Yuan Luo..
      </div>
      <div class="pvenue" style="padding:1px;margin-left:215px;">
          IEEE Transactions on Image Processing (<b>TIP</b>), 2025.
          <p>
            [<a href="https://ieeexplore.ieee.org/document/11049899">paper</a>]
            [<a href="https://arxiv.org/pdf/2309.16460.pdf">arxiv</a>]
            [<a href="https://github.com/longshaocong/DTCS">code</a>]
            [<a href="bibs/Arxiv_2024_DTCS.txt">bib</a>] 
          </p>
      </div>
    </div>

    <div class="paper" style="clear:left;">
      <div class="pimg" style="float:left;margin-bottom:10px;padding-left:3px;border-bottom:solid 1px #EEE;box-shadow:0 0px 2px"><font face="Arial" size="3">
          <img src="imgs/fig_pcotta_nips_24.png" width="200" >
      </div>
      <div class="ptitle" style="padding:1px;margin-left:215px;">
          PCoTTA: Continual Test-Time Adaptation for Multi-Task Point Cloud Understanding
      </div>
      <div class="pauthors" style="padding:1px;margin-left:215px;">
          Jincen Jiang*, <b>Qianyu Zhou*</b>, Yuhang Li, Xuequan Lu, Xinkui Zhao, Meili Wang, Lizhuang Ma, Jian Chang, Jian Jun Zhang.
      </div>
      <div class="pvenue" style="padding:1px;margin-left:215px;">
          The 38th Annual Conference on Neural Information Processing Systems (<b>NeurIPS</b>), 2024.
          <p>
             [<a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/ae8b0b5838ba510daff1198474e7b984-Paper-Conference.pdf">paper</a>]
             [<a href="https://arxiv.org/pdf/2411.00632">arxiv</a>]
             [<a href="https://github.com/Jinec98/PCoTTA">code</a>]
             [<a href="bibs/NeurIPS_2024_PCoTTA.txt">bib</a>] 
          </p>
      </div>
    </div>

    <div class="paper" style="clear:left;">
      <div class="pimg" style="float:left;margin-bottom:10px;padding-left:3px;border-bottom:solid 1px #EEE;box-shadow:0 0px 2px"><font face="Arial" size="3">
          <img src="imgs/fig_pdgmamba_arxiv_24.png" width="200" >
      </div>
      <div class="ptitle" style="padding:1px;margin-left:215px;">
          PointDGMamba: Domain Generalization of Point Cloud Classification via Generalized State Space Model
      </div>
      <div class="pauthors" style="padding:1px;margin-left:215px;">
          Hao Yang*, <b>Qianyu Zhou*</b>, Haijia Sun, Xiangtai Li, Fengqi Liu, Xuequan Lu, Lizhuang Ma, Shuicheng Yan. 
      </div>
      <div class="pvenue" style="padding:1px;margin-left:215px;">
          AAAI Conference on Artificial Intelligence (<b>AAAI</b>), 2025.
          <p>
            [<a href="https://arxiv.org/pdf/2408.13574">paper</a>]
            [<a href="https://arxiv.org/pdf/2408.13574">arxiv</a>]
            [<a href="https://github.com/yxltya/PointDGMamba">code</a>]
            [<a href="https://doi.org/10.1609/aaai.v39i9.32995">DOI</a>]
            [<a href="bibs/AAAI_2025_PDGMamba.txt">bib</a>] 
          </p>
      </div>
    </div>

    <div class="paper" style="clear:left;">
      <div class="pimg" style="float:left;margin-bottom:10px;padding-left:3px;border-bottom:solid 1px #EEE;box-shadow:0 0px 2px"><font face="Arial" size="3">
          <img src="imgs/fig_da_pointr_aaai_25.png" width="200" >
      </div>
      <div class="ptitle" style="padding:1px;margin-left:215px;">
          DAPoinTr: Domain Adaptive Point Transformer for Point Cloud Completion
      </div>
      <div class="pauthors" style="padding:1px;margin-left:215px;">
          Yinghui Li*, <b>Qianyu Zhou*</b>,  Jingyu Gong, Ye Zhu, Richard Dazeley, Xinkui Zhao, Xuequan Lu. 
      </div>
      <div class="pvenue" style="padding:1px;margin-left:215px;">
          AAAI Conference on Artificial Intelligence (<b>AAAI</b>), 2025.
          <p>
            [<a href="https://arxiv.org/pdf/2412.19062">paper</a>]
            [<a href="https://arxiv.org/pdf/2412.19062">arxiv</a>]
            [<a href="https://github.com/Yinghui-Li-New/DAPoinTr">code</a>]
            [<a href="https://doi.org/10.1609/aaai.v39i5.32537">DOI</a>]
            [<a href="bibs/AAAI_2025_DAPoinTr.txt">bib</a>]  
          </p>
      </div>
    </div>
    <div class="paper" style="clear:left;">
      <div class="pimg" style="float:left;margin-bottom:10px;padding-left:3px;border-bottom:solid 1px #EEE;box-shadow:0 0px 2px"><font face="Arial" size="3">
          <img src="imgs/fig_basam_cvpr_24.png" width="200" >
      </div>
      <div class="ptitle" style="padding:1px;margin-left:215px;">
          BA-SAM: Scalable Bias-Mode Attention Mask for Segment Anything Model
      </div>
      <div class="pauthors" style="padding:1px;margin-left:215px;">
          Yiran Song*, <b>Qianyu Zhou*</b>, Xiangtai Li, Deng-Ping Fan, Xuequan Lu, Lizhuang Ma.
      </div>
      <div class="pvenue" style="padding:1px;margin-left:215px;">
          IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2024.
          <p>
            [<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Song_BA-SAM_Scalable_Bias-Mode_Attention_Mask_for_Segment_Anything_Model_CVPR_2024_paper.pdf">paper</a>]
            [<a href="https://openaccess.thecvf.com/content/CVPR2024/supplemental/Song_BA-SAM_Scalable_Bias-Mode_CVPR_2024_supplemental.pdf">supp</a>]
            [<a href="https://arxiv.org/pdf/2401.02317.pdf">arxiv</a>]
            [<a href="https://doi.org/10.1109/CVPR52733.2024.00305">DOI</a>]
            [<a href="https://github.com/zongzi13545329/BA-SAM">code</a>]
            [<a href="bibs/CVPR_2024_BA-SAM.txt">bib</a>] 
          </p>
      </div>
    </div>


    <div class="paper" style="clear:left;">
      <div class="pimg" style="float:left;margin-bottom:10px;padding-left:3px;border-bottom:solid 1px #EEE;box-shadow:0 0px 2px"><font face="Arial" size="3">
          <img src="imgs/fig_dgmamba_mm_24.png" width="200" >
      </div>
      <div class="ptitle" style="padding:1px;margin-left:215px;">
          DGMamba: Domain Generalization via Generalized State Space Model
      </div>
      <div class="pauthors" style="padding:1px;margin-left:215px;">
          Shaocong Long*, <b>Qianyu Zhou*</b>, Xiangtai Li, Xuequan Lu, Chenhao Ying, Yuan Luo, Lizhuang Ma, Shuicheng Yan.
      </div>
      <div class="pvenue" style="padding:1px;margin-left:215px;">
          The 32nd ACM International Conference on Multimedia (<b>ACM MM</b>), 2024.
          <p>
            [<a href="https://arxiv.org/pdf/2404.07794.pdf">paper</a>]
            [<a href="https://arxiv.org/pdf/2404.07794.pdf">arxiv</a>]
            [<a href="https://dl.acm.org/doi/10.1145/3664647.3681247">DOI</a>]
            [<a href="https://github.com/longshaocong/DGMamba">code</a>]  
            [<a href="bibs/MM_2024_DGMamba.txt">bib</a>] 
          </p>
      </div>
    </div>


    <div class="paper" style="clear:left;">
      <div class="pimg" style="float:left;margin-bottom:10px;padding-left:3px;border-bottom:solid 1px #EEE;box-shadow:0 0px 2px"><font face="Arial" size="3">
          <img src="imgs/fig_adv_pruning_mm_24.png" width="200" >
      </div>
      <div class="ptitle" style="padding:1px;margin-left:215px;">
          Rethinking Impersonation and Dodging Attacks on Face Recognition Systems
      </div>
      <div class="pauthors" style="padding:1px;margin-left:215px;">
          Fengfan Zhou*, <b>Qianyu Zhou*</b>, Bangjie Yin, Hui Zheng, Xuequan Lu, Lizhuang Ma, Hefei Ling.
      </div>
      <div class="pvenue" style="padding:1px;margin-left:215px;">
          The 32nd ACM International Conference on Multimedia (<b>ACM MM</b>), 2024.
          <p>
            [<a href="https://arxiv.org/pdf/2401.08903">paper</a>]
            [<a href="https://arxiv.org/pdf/2401.08903">arxiv</a>]
            [<a href="https://dl.acm.org/doi/10.1145/3664647.3681440">DOI</a>]
            [<a href="https://github.com/fengfanzhou/Adv-Pruning">code</a>]
            [<a href="bibs/MM_2024_Adv_Pruning.txt">bib</a>]
          </p>
      </div>
    </div>


    <div class="paper" style="clear:left;">
      <div class="pimg" style="float:left;margin-bottom:10px;padding-left:3px;border-bottom:solid 1px #EEE;box-shadow:0 0px 2px"><font face="Arial" size="3">
          <img src="imgs/fig_dgpic_eccv_24.png" width="200" >
      </div>
      <div class="ptitle" style="padding:1px;margin-left:215px;">
          DG-PIC: Domain Generalized Point-In-Context Learning for Point Cloud Understanding
      </div>
      <div class="pauthors" style="padding:1px;margin-left:215px;">
          Jincen Jiang*, <b>Qianyu Zhou*</b>, Yuhang Li, Xuequan Lu, Meili Wang, Lizhuang Ma, Jian Chang, Jian Jun Zhang.
      </div>
      <div class="pvenue" style="padding:1px;margin-left:215px;">
          European Conference on Computer Vision (<b>ECCV</b>), 2024.
          <p>
             [<a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01036.pdf">paper</a>]
             [<a href="https://arxiv.org/pdf/2407.08801">arxiv</a>]
             [<a href="https://link.springer.com/chapter/10.1007/978-3-031-72658-3_26">DOI</a>]
             [<a href="https://github.com/Jinec98/DG-PIC">code</a>]
             [<a href="bibs/ECCV_2024_DGPIC.txt">bib</a>] 
          </p>
      </div>
    </div>


    <div class="paper" style="clear:left;">
      <div class="pimg" style="float:left;margin-bottom:10px;padding-left:3px;border-bottom:solid 1px #EEE;box-shadow:0 0px 2px"><font face="Arial" size="3">
          <img src="imgs/fig_dmda_tcsvt_24.png" width="200" >
      </div>
      <div class="ptitle" style="padding:1px;margin-left:215px;">
          Rethinking Domain Generalization: Discriminability and Generalizability
      </div>
      <div class="pauthors" style="padding:1px;margin-left:215px;">
          Shaocong Long*, <b>Qianyu Zhou*</b>, Chenhao Ying, Lizhuang Ma, Yuan Luo.
      </div>
      <div class="pvenue" style="padding:1px;margin-left:215px;">
          IEEE Transactions on Circuits and Systems for Video Technology (<b>TCSVT</b>), 2024.
          <p>
            [<a href="https://ieeexplore.ieee.org/document/10583916">paper</a>]
            [<a href="https://arxiv.org/pdf/2309.16483.pdf">arxiv</a>]
            [<a href="https://doi.org/10.1109/TCSVT.2024.3422887">DOI</a>]
            [<a href="https://github.com/longshaocong/DMDA">code</a>]
            [<a href="bibs/TCSVT_2024_DMDA.txt">bib</a>] 
          </p>
      </div>
    </div>


    <div class="paper" style="clear:left;">
      <div class="pimg" style="float:left;margin-bottom:10px;padding-left:3px;border-bottom:solid 1px #EEE;box-shadow:0 0px 2px"><font face="Arial" size="3">
          <img src="imgs/fig_pit_iccv_21.png" width="200" >
      </div>
      <div class="ptitle" style="padding:1px;margin-left:215px;">
          PIT: Position-Invariant Transform for Cross-FoV Domain Adaptation
      </div>
      <div class="pauthors" style="padding:1px;margin-left:215px;">
          Qiqi Gu*, <b>Qianyu Zhou*</b>, Minghao Xu, Zhengyang Feng, Guangliang Cheng, Xuequan Lu, Jianping Shi, Lizhuang Ma.
      </div>
      <div class="pvenue" style="padding:1px;margin-left:215px;">
          IEEE/CVF International Conference on Computer Vision (<b>ICCV</b>), 2021.
          <p>
           [<a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Gu_PIT_Position-Invariant_Transform_for_Cross-FoV_Domain_Adaptation_ICCV_2021_paper.pdf">paper</a>]
           [<a href="https://arxiv.org/pdf/2108.07142.pdf">arxiv</a>]
           [<a href="https://ieeexplore.ieee.org/document/9711500">DOI</a>] 
           [<a href="https://github.com/sheepooo/PIT-Position-Invariant-Transform">code</a>]
           [<a href="bibs/ICCV_2021_PIT.txt">bib</a>]
          </p>
      </div>
    </div>


    <div class="paper" style="clear:left;">
      <div class="pimg" style="float:left;margin-bottom:10px;padding-left:3px;border-bottom:solid 1px #EEE;box-shadow:0 0px 2px"><font face="Arial" size="3">
          <img src="imgs/fig_transvod_mm_21.png" width="200" >
      </div>
      <div class="ptitle" style="padding:1px;margin-left:215px;">
          End-to-End Video Object Detection with Spatial-Temporal Transformers
      </div>
      <div class="pauthors" style="padding:1px;margin-left:215px;">
          Lu He*, <b>Qianyu Zhou*</b>, Xiangtai Li*, Li Niu, Guangliang Cheng, Xiao Li, Wenxuan Liu, Yunhai Tong, Lizhuang Ma, Liqing Zhang.
      </div>
      <div class="pvenue" style="padding:1px;margin-left:215px;">
          The 29th ACM International Conference on Multimedia (<b>ACM MM</b>), 2021.
          <p>
           [<a href="https://dl.acm.org/doi/pdf/10.1145/3474085.3475285">paper</a>]
           [<a href="https://arxiv.org/pdf/2105.10920.pdf">arxiv</a>]
           [<a href="https://dl.acm.org/doi/10.1145/3474085.3475285">DOI</a>]
           [<a href="https://github.com/SJTU-LuHe/TransVOD">code</a>] 
           [<a href="bibs/MM_2021_TransVOD.txt">bib</a>]
          </p>
      </div>
    </div>

    <div class="paper" style="clear:left;">
      <div class="pimg" style="float:left;margin-bottom:10px;padding-left:3px;border-bottom:solid 1px #EEE;box-shadow:0 0px 2px"><font face="Arial" size="3">
          <img src="imgs/fig_dmt_pr_22.png" width="200" >
      </div>
      <div class="ptitle" style="padding:1px;margin-left:215px;">
          DMT: Dynamic Mutual Training for Semi-Supervised Learning
      </div>
      <div class="pauthors" style="padding:1px;margin-left:215px;">
          Zhengyang Feng*, <b>Qianyu Zhou*</b>, Qiqi Gu, Xin Tan, Guangliang Cheng, Xuequan Lu, Jianping Shi, Lizhuang Ma.
      </div>
      <div class="pvenue" style="padding:1px;margin-left:215px;">
          Patter Recognition (<b>PR</b>), 2022.
          <p>
            [<a href="https://www.sciencedirect.com/science/article/pii/S0031320322002588">paper</a>]
               [<a href="https://arxiv.org/pdf/2004.08514.pdf">arxiv</a>]
               [<a href="https://www.sciencedirect.com/science/article/pii/S0031320322002588">DOI</a>]
               [<a href="https://github.com/voldemortX/DST-CBC">code</a>]
               [<a href="bibs/PR_2022_DMT.txt">bib</a>]
          </p>
      </div>
    </div>

    <div class="paper" style="clear:left;">
      <div class="pimg" style="float:left;margin-bottom:10px;padding-left:3px;border-bottom:solid 1px #EEE;box-shadow:0 0px 2px"><font face="Arial" size="3">
          <img src="imgs/fig_cpabmm_aaai_24.png" width="200" >
      </div>
      <div class="ptitle" style="padding:1px;margin-left:215px;">
          Continuous Piecewise-Affine Based Motion Model for Image Animation
      </div>
      <div class="pauthors" style="padding:1px;margin-left:215px;">
          Hexiang Wang*, Fengqi Liu*, <b>Qianyu Zhou*</b>, Xin Tan, Ran Yi, Lizhuang Ma.
      </div>
      <div class="pvenue" style="padding:1px;margin-left:215px;">
          AAAI Conference on Artificial Intelligence (<b>AAAI</b>), 2024.
          <p>
            [<a href="https://arxiv.org/pdf/2401.09146.pdf">paper</a>]
            [<a href="https://arxiv.org/pdf/2401.09146.pdf">arxiv</a>]
            [<a href="https://github.com/DevilPG/AAAI2024-CPABMM">code</a>]
            [<a href="bibs/AAAI_2024_CPABMM.txt">bib</a>] 
          </p>
      </div>
    </div>

    <div class="paper" style="clear:left;">
      <div class="pimg" style="float:left;margin-bottom:1px;padding-left:3px;border-bottom:solid 1px #EEE;box-shadow:0 0px 2px"><font face="Arial" size="3">
          <img src="imgs/fig_cloudmix_tvcg_24.png" width="200" >
      </div>
      <div class="ptitle" style="padding:1px;margin-left:215px;">
          CloudMix: Dual Mixup Consistency for Unpaired Point Cloud Completion
      </div>
      <div class="pauthors" style="padding:1px;margin-left:215px;">
          Fengqi Liu*, Jingyu Gong*, <b>Qianyu Zhou</b>, Xuequan Lu, Ran Yi, Yuan Xie, and Lizhuang Ma
      </div>
      <div class="pvenue" style="padding:1px;margin-left:215px;">
          IEEE Transactions on Visualization and Computer Graphics (<b>TVCG</b>), 2024.
          <p>
             [<a href="https://ieeexplore.ieee.org/document/10487003">paper</a>]
             [<a href="https://doi.org/10.1109/TVCG.2024.3383434">DOI</a>]
             [<a href="bibs/TVCG_2024_CloudMix.txt">bib</a>] 
          </p>
      </div>
    </div>

</ul>
</div>
</script>

<br>
<script id="pubs_by_topic">
Summary(38): TPAMI(2)+TIP(1), CVPR/ICCV/ECCV(4/2/3), NeurIPS(2), AAAI/IJCAI/ACM MM(4/1/5), TVCG(1), TCSVT/PR(3), others(10);</p>


<h3>2025:</h3>   
<ul>
    <li> <p style="margin-left: 0px; line-height: 150%; margin-top: 10px; margin-bottom: 10px;"><font face="Arial" size="3"><meta charset="utf-8">
            Diverse Target and Contribution Scheduling for Domain Generalization <br> 
         <i>   Shaocong Long*, <b>Qianyu Zhou*</b>, Chenhao Ying, Lizhuang Ma, Yuan Luo. </i><br>
         IEEE Transactions on Image Processing  (<b>TIP</b>), 2025 <b><font color="red">[CCF A]</font></b>  <br>
        [<a href="https://arxiv.org/pdf/2309.16460.pdf">paper</a>]
        [<a href="https://arxiv.org/pdf/2309.16460.pdf">arxiv</a>]
        [<a href="https://github.com/longshaocong/DTCS">code</a>]
        [<a href="bibs/Arxiv_2024_DTCS.txt">bib</a>]     
        </font>
    </p> </li>

    <li> <p style="margin-left: 0px; line-height: 150%; margin-top: 10px; margin-bottom: 10px;"><font face="Arial" size="3"><meta charset="utf-8">
            PointDGMamba: Domain Generalization of Point Cloud Classification via Generalized State Space Model <br> 
         <i>   Hao Yang*, <b>Qianyu Zhou*</b>, Haijia Sun, Xiangtai Li, Fengqi Liu, Xuequan Lu, Lizhuang Ma, Shuicheng Yan. </i><br> 
           AAAI Conference on Artificial Intelligence (<b>AAAI</b>), 2025 <b><font color="red">[CCF A]</font></b> <br>
            [<a href="https://arxiv.org/pdf/2408.13574">paper</a>]
            [<a href="https://arxiv.org/pdf/2408.13574">arxiv</a>]
            [<a href="https://github.com/yxltya/PointDGMamba">code</a>]
            [<a href="https://doi.org/10.1609/aaai.v39i9.32995">DOI</a>]
            [<a href="bibs/AAAI_2025_PDGMamba.txt">bib</a>]  
      </font>
     </p> </li>
   
    <li> <p style="margin-left: 0px; line-height: 150%; margin-top: 10px; margin-bottom: 10px;"><font face="Arial" size="3"><meta charset="utf-8">
            DAPoinTr: Domain Adaptive Point Transformer for Point Cloud Completion  <br> 
     <i>   Yinghui Li*, <b>Qianyu Zhou*</b>,  Jingyu Gong, Ye Zhu, Richard Dazeley, Xinkui Zhao, Xuequan Lu. </i><br> 
           AAAI Conference on Artificial Intelligence (<b>AAAI</b>), 2025 <b><font color="red">[CCF A]</font></b> <br>
            [<a href="https://arxiv.org/pdf/2412.19062">paper</a>]
            [<a href="https://arxiv.org/pdf/2412.19062">arxiv</a>]
            [<a href="https://github.com/Yinghui-Li-New/DAPoinTr">code</a>]
            [<a href="https://doi.org/10.1609/aaai.v39i5.32537">DOI</a>]
            [<a href="bibs/AAAI_2025_DAPoinTr.txt">bib</a>]  
   </font>
     </p> </li>

    <li> <p style="margin-left: 0px; line-height: 150%; margin-top: 10px; margin-bottom: 10px;"><font face="Arial" size="3"><meta charset="utf-8">
            Point Cloud Mamba: Point Cloud Learning via State Space Model <br> 
         <i>  Tao Zhang, Haobo Yuan, Lu Qi, Jiangning Zhang, <b>Qianyu Zhou</b>, Shunping Ji, Shuicheng YAN, Xiangtai Li . </i><br> 
           AAAI Conference on Artificial Intelligence (<b>AAAI</b>), 2025 <b><font color="red">[CCF A]</font></b> <br>
            [<a href="https://arxiv.org/pdf/2403.00762">paper</a>]
            [<a href="https://arxiv.org/pdf/2403.00762">arxiv</a>]
            [<a href="https://github.com/zhang-tao-whu/PCM">code</a>]
            [<a href="https://doi.org/10.1609/aaai.v39i10.33098">DOI</a>]
            [<a href="bibs/AAAI_2025_PCM.txt">bib</a>] 
   </font>
   </p> </li>

    <li> <p style="margin-left: 0px; line-height: 150%; margin-top: 10px; margin-bottom: 10px;"><font face="Arial" size="3"><meta charset="utf-8">
            Are They the Same? Exploring Visual Correspondence Shortcomings of Multimodal LLMs <br> 
         <i>   Yikang Zhou, Tao Zhang, Shilin Xu, Shihao Chen, <b>Qianyu Zhou</b>, Yunhai Tong, Shunping Ji, Jiangning Zhang, Xiangtai Li, Lu Qi. </i><br>
        IEEE/CVF International Conference on Computer Vision (<b>ICCV</b>), 2025 <b><font color="red">[CCF A]</font></b> <br>
            [<a href="https://arxiv.org/abs/2501.04670">paper</a>]
            [<a href="https://arxiv.org/abs/2501.04670">arxiv</a>]
            [<a href="https://github.com/zhouyiks/CoLVA">code</a>]
            [<a href="bibs/Arxiv_2025_MMVM.txt">bib</a>]
        </font>

    </p> </li>

    <li> <p style="margin-left: 0px; line-height: 150%; margin-top: 10px; margin-bottom: 10px;"><font face="Arial" size="3"><meta charset="utf-8">
            Enhancing the Transferability of Adversarial Attacks on Face Recognition with Diverse Parameters Augmentation <br> 
         <i>   Fengfan Zhou, Bangjie Yin, Hefei Ling, <b>Qianyu Zhou</b>, Wenxuan Wang. </i><br>
        IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2025 <b><font color="red">[CCF A]</font></b> <br>
        [<a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Zhou_Improving_the_Transferability_of_Adversarial_Attacks_on_Face_Recognition_with_CVPR_2025_paper.pdf">paper</a>]
        [<a href="https://arxiv.org/pdf/2411.15555">arxiv</a>]
        [<a href="https://openaccess.thecvf.com/content/CVPR2025/supplemental/Zhou_Improving_the_Transferability_CVPR_2025_supplemental.pdf">supp</a>]
        [<a href="bibs/CVPR_2025_SDB.txt">bib</a>]
        </font>

    </p> </li>

        <li> <p style="margin-left: 0px; line-height: 150%; margin-top: 10px; margin-bottom: 10px;"><font face="Arial" size="3"><meta charset="utf-8">
            CloudMix: Dual Mixup Consistency for Unpaired Point Cloud Completion <br> 
         <i>   Fengqi Liu*, Jingyu Gong*, <b>Qianyu Zhou</b>, Xuequan Lu, Ran Yi, Yuan Xie, and Lizhuang Ma </i><br>
         IEEE Transactions on Visualization and Computer Graphics (<b>TVCG</b>), 2025 <b><font color="red">[CCF A, JCR Q1]</font></b> <br>
         [<a href="https://ieeexplore.ieee.org/document/10487003">paper</a>]
         [<a href="https://doi.org/10.1109/TVCG.2024.3383434">DOI</a>]
         [<a href="bibs/TVCG_2024_CloudMix.txt">bib</a>]   
        </font>
        </p> </li>

        <li> <p style="margin-left: 0px; line-height: 150%; margin-top: 10px; margin-bottom: 10px;"><font face="Arial" size="3"><meta charset="utf-8">
            Adversarial Attacks on Both Face Recognition and Face Anti-spoofing Models <br> 
         <i>   Fengfan Zhou, <b>Qianyu Zhou</b>, Xuequan Lu, Lizhuang Ma, Hefei Ling. </i><br>
         The 34th International Joint Conference on Artificial Intelligence (<b>IJCAI</b>), 2025. <b><font color="red">[CCF A]</font></b>
            <br>
        [<a href="https://arxiv.org/pdf/2405.16940">paper</a>]
        [<a href="https://arxiv.org/pdf/2405.16940">arxiv</a>]
        [<a href="bibs/Arxiv_2024_SDB.txt">bib</a>]
        </font>
        </p> </li>

    <li> <p style="margin-left: 0px; line-height: 150%; margin-top: 10px; margin-bottom: 10px;"><font face="Arial" size="3"><meta charset="utf-8">
            DiffuseFIST: A Fast Image-guided Style Transfer Method for Adapting Large-scale Diffusion Models <br> 
         <i>  Miaomiao Dai, <b>Qianyu Zhou</b>, Ran Yi, Lizhuang Ma . </i><br> 
           IEEE International Conference on Acoustics, Speech and Signal Processing (<b>ICASSP</b>), 2025 <b><font color="red">[CCF B]</font></b> <br>
            [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10889203">paper</a>]
            [<a href="https://ieeexplore.ieee.org/document/10889203">DOI</a>]
            [<a href="bibs/ICASSP_2025_DiffuseFIST.txt">bib</a>] 
   </font>
   </p> </li>

    <li> <p style="margin-left: 0px; line-height: 150%; margin-top: 10px; margin-bottom: 10px;"><font face="Arial" size="3"><meta charset="utf-8">
            SU-SAM: A Simple Unified Framework for Adapting Segment Anything Model in Underperformed Scenes <br> 
         <i>   Yiran Song*, <b>Qianyu Zhou*</b>, Xuequan Lu, Zhiwen Shao, Lizhuang Ma. </i><br>
        IEEE International Conference on Multimedia and Expo (<b>ICME</b>), 2025 <b><font color="red">[CCF B]</font></b> <br>
        [<a href="https://arxiv.org/pdf/2401.17803.pdf">paper</a>]
        [<a href="https://arxiv.org/pdf/2401.17803.pdf.pdf">arxiv</a>]
        [<a href="https://github.com/zongzi13545329/SimAda">code</a>] 
        [<a href="bibs/ICME_2025_SUSAM.txt">bib</a>]
    </font>
    </p> </li>

    <li> <p style="margin-left: 0px; line-height: 150%; margin-top: 10px; margin-bottom: 10px;"><font face="Arial" size="3"><meta charset="utf-8">
            Domain Generalization via Discrete Codebook Learning <br> 
         <i>   Shaocong Long*, <b>Qianyu Zhou*</b>, Xikun Jiang, Chenhao Ying, Lizhuang Ma, Yuan Luo. </i><br>
        IEEE International Conference on Multimedia and Expo (<b>ICME</b>), 2025 <b><font color="red">[CCF B]</font></b> <br>
        [<a href="">paper</a>]
        [<a href="">arxiv</a>]
        [<a href="bibs/ICME_2025_DDG.txt">bib</a>]
    </font>
    </p> </li>

    <li> <p style="margin-left: 0px; line-height: 150%; margin-top: 10px; margin-bottom: 10px;"><font face="Arial" size="3"><meta charset="utf-8">
            StyleRWKV: High-Quality and High-Efficiency Style Transfer with RWKV-like Architecture <br> 
         <i>   Miaomiao Dai*, <b>Qianyu Zhou*</b>, Lizhuang Ma. </i><br>
        IEEE International Conference on Multimedia and Expo (<b>ICME</b>), 2025 <b><font color="red">[CCF B]</font></b> <br>
        [<a href="https://arxiv.org/pdf/2412.19535">paper</a>]
        [<a href="https://arxiv.org/pdf/2412.19535">arxiv</a>]
        [<a href="bibs/ICME_2025_StyleRWKV.txt">bib</a>]
    </font>
    </p> </li>

</ul>

<h3>2024:</h3>
<ul>

    <li> <p style="margin-left: 0px; line-height: 150%; margin-top: 10px; margin-bottom: 10px;"><font face="Arial" size="3"><meta charset="utf-8">
            Test-Time Domain Generalization for Face Anti-Spoofing <br> 
         <i>   <b>Qianyu Zhou*</b>, Ke-Yue Zhang*, Taiping Yao, Xuequan Lu, Shouhong Ding, Lizhuang Ma. </i><br> 
           IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), pp. 175-187, June, 2024 <b><font color="red">[CCF A]</font></b> <br>
         [<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhou_Test-Time_Domain_Generalization_for_Face_Anti-Spoofing_CVPR_2024_paper.pdf">paper</a>]
         [<a href="https://arxiv.org/pdf/2403.19334.pdf">arxiv</a>]
         [<a href="https://doi.org/10.1109/CVPR52733.2024.00025">DOI</a>]
         [<a href="bibs/CVPR_2024_TTDG.txt">bib</a>] 
      </font>
     </p> </li>

    <li> <p style="margin-left: 0px; line-height: 150%; margin-top: 10px; margin-bottom: 10px;"><font face="Arial" size="3"><meta charset="utf-8">
            BA-SAM: Scalable Bias-Mode Attention Mask for Segment Anything Model <br> 
         <i>   Yiran Song*, <b>Qianyu Zhou*</b>, Xiangtai Li, Deng-Ping Fan, Xuequan Lu, Lizhuang Ma. </i><br>
         IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), June, 2024 <b><font color="red">[CCF A]</font></b> <br>
        [<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Song_BA-SAM_Scalable_Bias-Mode_Attention_Mask_for_Segment_Anything_Model_CVPR_2024_paper.pdf">paper</a>]
        [<a href="https://openaccess.thecvf.com/content/CVPR2024/supplemental/Song_BA-SAM_Scalable_Bias-Mode_CVPR_2024_supplemental.pdf">supp</a>]
        [<a href="https://arxiv.org/pdf/2401.02317.pdf">arxiv</a>]
        [<a href="https://github.com/zongzi13545329/BA-SAM">code</a>]
        [<a href="https://doi.org/10.1109/CVPR52733.2024.00305">DOI</a>]
        [<a href="bibs/CVPR_2024_BA-SAM.txt">bib</a>]  
        </font>
        </p> </li>

       <li> <p style="margin-left: 0px; line-height: 150%; margin-top: 10px; margin-bottom: 10px;"><font face="Arial" size="3"><meta charset="utf-8">
            PCoTTA: Continual Test-Time Adaptation for Multi-Task Point Cloud Understanding <br> 
        <i> Jincen Jiang*, <b>Qianyu Zhou*</b>, Yuhang Li, Xuequan Lu, Xinkui Zhao, Meili Wang, Lizhuang Ma, Jian Chang, Jian Jun Zhang</i><br>
         The 38th Annual Conference on Neural Information Processing Systems (<b>NeurIPS</b>), 2024. <b><font color="red">[CCF A]</font></b> <br>
        [<a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/ae8b0b5838ba510daff1198474e7b984-Paper-Conference.pdf">paper</a>]
        [<a href="https://arxiv.org/pdf/2411.00632">arxiv</a>]
        [<a href="https://github.com/Jinec98/PCoTTA">code</a>] 
        [<a href="bibs/NeurIPS_2024_PCoTTA.txt">bib</a>]
        </font>
        </p> </li>

        <li> <p style="margin-left: 0px; line-height: 150%; margin-top: 10px; margin-bottom: 10px;"><font face="Arial" size="3"><meta charset="utf-8">
            MotionBooth: Motion-Aware Customized Text-to-Video Generation <br> 
         <i> Jianzong Wu, Xiangtai Li, Yanhong Zeng, Jiangning Zhang, <b>Qianyu Zhou</b>, Yining Li, Yunhai Tong, Kai Chen. </i><br>
         The 38th Annual Conference on Neural Information Processing Systems (<b>NeurIPS</b>), 2024. <b><font color="red">[CCF A]</font></b> <br>
        [<a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/3cbf33008024aa1612ce853ef78e0e53-Paper-Conference.pdf">paper</a>]
        [<a href="https://arxiv.org/pdf/2406.17758">arxiv</a>]
        [<a href="https://github.com/jianzongwu/MotionBooth">code</a>] 
        [<a href="bibs/NeurIPS_2024_MotionBooth.txt">bib</a>]
        </font>
        </p> </li>

    <li> <p style="margin-left: 0px; line-height: 150%; margin-top: 10px; margin-bottom: 10px;"><font face="Arial" size="3"><meta charset="utf-8">
            DGMamba: Domain Generalization via Generalized State Space Model <br> 
         <i>   Shaocong Long*, <b>Qianyu Zhou*</b>, Xiangtai Li, Xuequan Lu, Chenhao Ying, Yuan Luo, Lizhuang Ma, Shuicheng Yan. </i><br>
         The 32nd ACM International Conference on Multimedia (<b>ACM MM</b>), October, 2024 <b><font color="red">[CCF A]</font></b> <br>
        [<a href="https://dl.acm.org/doi/10.1145/3664647.3681247">paper</a>]
        [<a href="https://arxiv.org/pdf/2404.07794.pdf">arxiv</a>]
        [<a href="https://github.com/longshaocong/DGMamba">code</a>]
        [<a href="https://dl.acm.org/doi/10.1145/3664647.3681247">DOI</a>] 
        [<a href="bibs/MM_2024_DGMamba.txt">bib</a>]     
        </font>
        </p> </li>

        <li> <p style="margin-left: 0px; line-height: 150%; margin-top: 10px; margin-bottom: 10px;"><font face="Arial" size="3"><meta charset="utf-8">
            Rethinking Impersonation and Dodging Attacks on Face Recognition Systems <br> 
         <i>   Fengfan Zhou*, <b>Qianyu Zhou*</b>, Bangjie Yin, Hui Zheng, Xuequan Lu, Lizhuang Ma, Hefei Ling. </i><br>
        The 32nd ACM International Conference on Multimedia (<b>ACM MM</b>), October, 2024 <b><font color="red">[CCF A]</font></b> <br>
        [<a href="https://dl.acm.org/doi/10.1145/3664647.3681440">paper</a>]
        [<a href="https://arxiv.org/pdf/2401.08903">arxiv</a>]
        [<a href="https://github.com/fengfanzhou/Adv-Pruning">code</a>]
        [<a href="https://dl.acm.org/doi/10.1145/3664647.3681440">DOI</a>]
        [<a href="bibs/MM_2024_Adv_Pruning.txt">bib</a>]
        </font>
        </p> </li>

        <li> <p style="margin-left: 0px; line-height: 150%; margin-top: 10px; margin-bottom: 10px;"><font face="Arial" size="3"><meta charset="utf-8">
            Emphasizing Semantic Consistency of Salient Posture for Speech-Driven Gesture Generation <br> 
         <i>   Fengqi Liu, Hexiang Wang, Jingyu Gong, Ran Yi, <b>Qianyu Zhou</b>, Xuequan Lu, Jiangbo Lu, Lizhuang Ma. </i><br>
        The 32nd ACM International Conference on Multimedia (<b>ACM MM</b>), October, 2024 <b><font color="red">[CCF A]</font></b> <br>
        [<a href="https://dl.acm.org/doi/10.1145/3664647.3680892">paper</a>]
        [<a href="https://openreview.net/pdf?id=NaUbjH7QEL">arxiv</a>]
        [<a href="bibs/MM_24_SCSP.txt">bib</a>]
        </font>
        </p> </li>

    <li> <p style="margin-left: 0px; line-height: 150%; margin-top: 10px; margin-bottom: 10px;"><font face="Arial" size="3"><meta charset="utf-8">
            DG-PIC: Domain Generalized Point-In-Context Learning for Point Cloud Understanding <br> 
         <i>  Jincen Jiang*, <b>Qianyu Zhou*</b>, Yuhang Li, Xuequan Lu, Meili Wang, Lizhuang Ma, Jian Chang, Jian Jun Zhang. </i><br>
           European Conference on Computer Vision (<b>ECCV</b>), October, 2024 <b><font color="red">[CCF B, TH-CPL A]</font></b> <br>
         [<a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01036.pdf">paper</a>]
         [<a href="https://arxiv.org/pdf/2407.08801">arxiv</a>]
         [<a href="https://github.com/Jinec98/DG-PIC">code</a>]
         [<a href="https://link.springer.com/chapter/10.1007/978-3-031-72658-3_26">DOI</a>]
         [<a href="bibs/ECCV_2024_DGPIC.txt">bib</a>] 

      </font>
     </p> </li>

    <li> <p style="margin-left: 0px; line-height: 150%; margin-top: 10px; margin-bottom: 10px;"><font face="Arial" size="3"><meta charset="utf-8">
            TF-FAS: Twofold-Element Fine-Grained Semantic Guidance for Generalizable Face Anti-Spoofing <br> 
         <i>  Xudong Wang, Ke-Yue Zhang, Taiping Yao, <b>Qianyu Zhou</b>, Shouhong Ding, Pingyang Dai, Rongrong Ji. </i><br>
           European Conference on Computer Vision (<b>ECCV</b>), October, 2024 <b><font color="red">[CCF B, TH-CPL A]</font></b> <br>
         [<a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01098.pdf">paper</a>]
         [<a href="https://github.com/xudongww/TF-FAS">code</a>]
         [<a href="https://link.springer.com/chapter/10.1007/978-3-031-72667-5_9">DOI</a>]
         [<a href="bibs/ECCV_2024_TF_FAS.txt">bib</a>] 
      </font>
     </p> </li>

    <li> <p style="margin-left: 0px; line-height: 150%; margin-top: 10px; margin-bottom: 10px;"><font face="Arial" size="3"><meta charset="utf-8">
            Rethinking Domain Generalization: Discriminability and Generalizability <br> 
         <i>   Shaocong Long*, <b>Qianyu Zhou*</b>, Chenhao Ying, Lizhuang Ma, Yuan Luo. </i><br>
        IEEE Transactions on Circuits and Systems for Video Technology (<b>TCSVT</b>), 2024 <b><font color="red">[CCF B, JCR Q1]</font></b> <br>
        [<a href="https://arxiv.org/pdf/2309.16483.pdf">paper</a>]
        [<a href="https://arxiv.org/pdf/2309.16483.pdf">arxiv</a>]
        [<a href="https://doi.org/10.1109/TCSVT.2024.3422887">DOI</a>]
        [<a href="https://github.com/longshaocong/DMDA">code</a>]
        [<a href="bibs/TCSVT_2024_DMDA.txt">bib</a>]     
        </font>
        </p> </li>

    <li> <p style="margin-left: 0px; line-height: 150%; margin-top: 10px; margin-bottom: 10px;"><font face="Arial" size="3"><meta charset="utf-8">
            Continuous Piecewise-Affine Based Motion Model for Image Animation <br> 
         <i>   Hexiang Wang*, Fengqi Liu*, <b>Qianyu Zhou*</b>, Xin Tan, Ran Yi, Lizhuang Ma. </i><br>
         AAAI Conference on Artificial Intelligence (<b>AAAI</b>), 2024 <b><font color="red">[CCF A]</font></b> <br>
        [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/28351/28688">paper</a>]
        [<a href="https://arxiv.org/pdf/2401.09146.pdf">arxiv</a>]
        [<a href="https://doi.org/10.1609/aaai.v38i6.28351">DOI</a>]
        [<a href="https://github.com/DevilPG/AAAI2024-CPABMM">code</a>]
        [<a href="bibs/AAAI_2024_CPABMM.txt">bib</a>]  
        </font>
        </p> </li>


    <li> <p style="margin-left: 0px; line-height: 150%; margin-top: 10px; margin-bottom: 10px;"><font face="Arial" size="3"><meta charset="utf-8">
            Source-free Test-Time Adaptation For Online Surface-Defect Detection <br> 
         <i>   Yiran Song, <b>Qianyu Zhou</b>, Lizhuang Ma. </i><br>
         The 27th International Conference on Pattern Recognition (<b>ICPR</b>), December, 2024 <b><font color="red">[CCF C]</font></b> <br>
        [<a href="https://link.springer.com/chapter/10.1007/978-3-031-78189-6_13">paper</a>]
        [<a href="https://arxiv.org/pdf/2408.09494">arxiv</a>]
        [<a href="bibs/ICPR_2024_SFTTA.txt">bib</a>]  
        </font>
        </p> </li>

    <li> <p style="margin-left: 0px; line-height: 150%; margin-top: 10px; margin-bottom: 10px;"><font face="Arial" size="3"><meta charset="utf-8">
            CFRL: Coarse-Fine Decoupled Representation Learning For Long-Tailed Recognition <br> 
         <i>   Yiran Song, <b>Qianyu Zhou</b>, Kun Hu, Lizhuang Ma, Xuequan Lu. </i><br>
        ACM Multimedia Asia 2024 (<b>MM Asia</b>), December, 2024 <b><font color="red">[CCF C]</font></b> <br>
        [<a href="https://dl.acm.org/doi/full/10.1145/3696409.3700195?casa_token=GQAfg9rHjg4AAAAA%3A38SYFY3QBdNA5V9oV6jozc-c6thmsstw1GVuUhSg0djS3wsi5aq_LXsRGzxKnAQ73sJwqwCSYTalrDc">paper</a>]
        [<a href="bibs/MMAsia_2024_CFRL.txt">bib</a>]  
        </font>
        </p> </li>

        <li> <p style="margin-left: 0px; line-height: 150%; margin-top: 10px; margin-bottom: 10px;"><font face="Arial" size="3"><meta charset="utf-8">
            Diffusion Implicit Policy for Unpaired Scene-aware Motion Synthesis <br> 
         <i>   Jingyu Gong, Chong Zhang, Fengqi Liu, Ke Fan, <b>Qianyu Zhou</b>, Xin Tan, Zhizhong Zhang, Yuan Xie, Lizhuang Ma. </i><br>
         ArXiv preprint arXiv:2412.02261. <br>
        [<a href="https://arxiv.org/pdf/2412.02261">paper</a>]
        [<a href="https://arxiv.org/pdf/2412.02261">arxiv</a>]
        [<a href="bibs/Arxiv_2024_DIP.txt">bib</a>]
        [<a href="https://jingyugong.github.io/DiffusionImplicitPolicy/">code</a>] 
        </font>
        </p> </li>

</ul> 
<h3>2023:</h3>    
<ul>

    <li> <p style="margin-left: 0px; line-height: 150%; margin-top: 10px; margin-bottom: 10px;"><font face="Arial" size="3"><meta charset="utf-8">
            TransVOD: End-to-End Video Object Detection with Spatial-Temporal Transformers <br> 
         <i>   <b>Qianyu Zhou*</b>, Xiangtai Li*, Lu He*, Yibo Yang, Guangliang Cheng, Yunhai Tong, Lizhuang Ma, Dacheng Tao. </i><br> 
         IEEE Transactions on Pattern Analysis and Machine Intelligence (<b>TPAMI</b>), vol. 45, no. 6, pp. 7853-7869, 2023 <b><font color="red">[CCF A]</font></b> <br>
   [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9960850">paper</a>]
   [<a href="https://arxiv.org/pdf/2201.05047.pdf">arxiv</a>] 
   [<a href="https://ieeexplore.ieee.org/document/9960850">DOI</a>]
   [<a href="https://github.com/SJTU-LuHe/TransVOD">code1</a>]
   [<a href="https://github.com/qianyuzqy/TransVOD_Lite">code2</a>]
   [<a href="https://github.com/qianyuzqy/TransVOD_plusplus">code3</a>]
   [<a href="bibs/TPAMI_2023_TransVOD.txt">bib</a>]
   </font>
     </p> </li>

    <li> <p style="margin-left: 0px; line-height: 150%; margin-top: 10px; margin-bottom: 10px;"><font face="Arial" size="3"><meta charset="utf-8">
            Self-Adversarial Disentangling for Specific Domain Adaptation <br> 
         <i>   <b>Qianyu Zhou</b>, Qiqi Gu, Jiangmiao Pang,  Xuequan Lu, Lizhuang Ma. </i><br> 
         IEEE Transactions on Pattern Analysis and Machine Intelligence (<b>TPAMI</b>), vol. 45, no. 7, pp. 8954-8968, 2023 <b><font color="red">[CCF A]</font></b> <br>
   [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10024368">paper</a>]
   [<a href="https://arxiv.org/pdf/2108.03553.pdf">arxiv</a>] 
   [<a href="https://ieeexplore.ieee.org/document/10024368">DOI</a>]
   [<a href="bibs/TPAMI_2023_SAD.txt">bib</a>]
   </font>
   </p> </li>

    <li> <p style="margin-left: 0px; line-height: 150%; margin-top: 10px; margin-bottom: 10px;"><font face="Arial" size="3"><meta charset="utf-8">
            Instance-Aware Domain Generalization for Face Anti-Spoofing <br> 
         <i>   <b>Qianyu Zhou*</b>, Ke-Yue Zhang*, Taiping Yao, Xuequan Lu, Ran Yi,  Shouhong Ding, Lizhuang Ma. </i><br> 
           IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), pp. 20453-20463, June, 2023 <b><font color="red">[CCF A]</font></b> <br>
     [<a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_Instance-Aware_Domain_Generalization_for_Face_Anti-Spoofing_CVPR_2023_paper.pdf">paper</a>]
     [<a href="https://arxiv.org/pdf/2304.05640.pdf">arxiv</a>] 
     [<a href="https://doi.org/10.1109/CVPR52729.2023.01959">DOI</a>]
     [<a href="https://github.com/qianyuzqy/IADG">code</a>]
     [<a href="bibs/CVPR_2023_IADG.txt">bib</a>]
      </font>
     </p> </li>

    <li> <p style="margin-left: 0px; line-height: 150%; margin-top: 10px; margin-bottom: 10px;"><font face="Arial" size="3"><meta charset="utf-8">
            Context-Aware Mixup for Domain Adaptive Semantic Segmentation <br> 
         <i>   <b>Qianyu Zhou</b>, Zhengyang Feng, Qiqi Gu, Jiangmiao Pang, Guangliang Cheng, Xuequan Lu, Jianping Shi, Lizhuang Ma. </i><br> 
           IEEE Transactions on Circuits and Systems for Video Technology (<b>TCSVT</b>), vol. 33, no. 2, pp. 804-817, 2023 <b><font color="red">[JCR Q1]</font></b> <br>
     [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9889681">paper</a>]
     [<a href="https://arxiv.org/pdf/2108.03557.pdf">arxiv</a>] 
     [<a href="https://ieeexplore.ieee.org/document/9889681">DOI</a>]
     [<a href="https://github.com/qianyuzqy/CAMix">code</a>]
     [<a href="bibs/TCSVT_2023_CAMix.txt">bib</a>]
     </font>
     </p> </li>
    

    <li> <p style="margin-left: 0px; line-height: 150%; margin-top: 10px; margin-bottom: 10px;"><font face="Arial" size="3"><meta charset="utf-8">
            Rethinking Implicit Neural Representations for Vision Learners <br> 
         <i>   Yiran Song, <b>Qianyu Zhou<sup>†</sup></b>, Lizhuang Ma<sup>†</sup>. </i><br> 
         IEEE International Conference on Acoustics, Speech and Signal Processing (<b>ICASSP</b>), June, 2023. <br>
         Top 3% Paper Recognition of all papers accepted at ICASSP 2023 <b><font color="red">[CCF B]</font></b> <br>
   [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10094875">paper</a>]
   [<a href="https://arxiv.org/pdf/2211.12040.pdf">arxiv</a>]
   [<a href="https://ieeexplore.ieee.org/document/10094875">DOI</a>]
   [<a href="bibs/ICASSP_2023_IRNS.txt">bib</a>]
   </font>
   </p> </li>

    <li> <p style="margin-left: 0px; line-height: 150%; margin-top: 10px; margin-bottom: 10px;"><font face="Arial" size="3"><meta charset="utf-8">
           ZDL: Zero-shot Degradation Factor Learning for Robust and Efficient Image Enhancement <br> 
         <i>   Hao Yang, Haijun Sun, <b>Qianyu Zhou</b>, Ran Yi, Lizhuang Ma. </i><br> 
         The 18th International Conference on Computer-Aided Design and Computer Graphics (<b>CCF CAD/Graphics</b>), August, 2023 <br>
         Best Paper Award in CAD/Graphics 2023. <b><font color="red">[CCF C]</font></b> <br>
   [<a href="https://link.springer.com/chapter/10.1007/978-981-99-9666-7_18">paper</a>]
   [<a href="https://link.springer.com/chapter/10.1007/978-981-99-9666-7_18">DOI</a>]
   [<a href="bibs/CADGraphics_2024_ZDL.txt">bib</a>]
   </font>
   </p> </li>

    <li> <p style="margin-left: 0px; line-height: 150%; margin-top: 10px; margin-bottom: 10px;"><font face="Arial" size="3"><meta charset="utf-8">
           Dynamic Sampling Dual Deformable Network for Online Video Instance Segmentation <br> 
           <i>   Yiran Song, <b>Qianyu Zhou</b>, Ran Yi, Zhiwen Shao, Lizhuang Ma. </i><br> 
         Journal of Zhejiang University (Engineering Science) in Chinese, 2023. <br>
   [<a href="">paper coming soon</a>]
   </font>
   </p> </li>


</ul>
<h3>2022:</h3>   
<ul>

    <li> <p style="margin-left: 0px; line-height: 150%; margin-top: 10px; margin-bottom: 10px;"><font face="Arial" size="3"><meta charset="utf-8">
            Adaptive Mixture of Experts Learning for Generalizable Face Anti-Spoofing <br> 
         <i>   <b>Qianyu Zhou*</b>, Ke-Yue Zhang*, Taiping Yao*, Ran Yi,  Shouhong Ding, Lizhuang Ma. </i><br> 
           The 30th ACM International Conference on Multimedia (<b>ACM MM</b>), pp. 6009-6018, October, 2022 <b><font color="red">[CCF A]</font></b> <br>
     [<a href="https://dl.acm.org/doi/pdf/10.1145/3503161.3547769">paper</a>]
     [<a href="https://arxiv.org/pdf/2207.09868.pdf">arxiv</a>]
     [<a href="https://dl.acm.org/doi/10.1145/3503161.3547769">DOI</a>]
     [<a href="bibs/MM_2022_AMEL.txt">bib</a>]
      </font>
     </p> </li>
   
    <li> <p style="margin-left: 0px; line-height: 150%; margin-top: 10px; margin-bottom: 10px;"><font face="Arial" size="3"><meta charset="utf-8">
            Generative Domain Adaptation for Face Anti-Spoofing  <br> 
     <i>   <b>Qianyu Zhou*</b>, Ke-Yue Zhang*, Taiping Yao, Ran Yi, Kekai Sheng, Shouhong Ding, Lizhuang Ma. </i><br> 
           European Conference on Computer Vision (<b>ECCV</b>), pp. 335-356, October, 2022 <b><font color="red">[CCF B, TH-CPL A]</font></b> <br>
     [<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136650328.pdf">paper</a>]
   [<a href="https://arxiv.org/pdf/2207.10015.pdf">arxiv</a>] 
   [<a href="https://link.springer.com/chapter/10.1007/978-3-031-20065-6_20">DOI</a>]
   [<a href="bibs/ECCV_2022_GDA.txt">bib</a>]
   </font>
     </p> </li>

    <li> <p style="margin-left: 0px; line-height: 150%; margin-top: 10px; margin-bottom: 10px;"><font face="Arial" size="3"><meta charset="utf-8">
            Uncertainty-Aware Consistency Regularization for Cross-Domain Semantic Segmentation <br> 
         <i>   <b>Qianyu Zhou</b>, Zhengyang Feng, Qiqi Gu, Guangliang  Cheng, Xuequan Lu, Jianping Shi, Lizhuang Ma. </i><br> 
           Computer Vision and Image Understanding (<b>CVIU</b>), vol. 221, no. C, pp. 103448-103460, 2022 <b><font color="red">[CCF B]</font></b> <br>
   [<a href="https://www.sciencedirect.com/science/article/pii/S1077314222000625">paper</a>]
   [<a href="https://arxiv.org/pdf/2004.08878.pdf">arxiv</a>]
   [<a href="https://www.sciencedirect.com/science/article/pii/S1077314222000625">DOI</a>]
   [<a href="bibs/CVIU_2022_UACR.txt">bib</a>] 
   </font>
   </p> </li>
  
    <li> <p style="margin-left: 0px; line-height: 150%; margin-top: 10px; margin-bottom: 10px;"><font face="Arial" size="3"><meta charset="utf-8">
            Domain Adaptive Semantic Segmentation via Regional Contrastive Consistency Regularization <br> 
         <i>   <b>Qianyu Zhou</b>, Chuyun Zhuang,  Ran Yi, Xuequan Lu, Lizhuang Ma. </i><br> 
         IEEE International Conference on Multimedia and Expo (<b>ICME</b>), July, 2022 (Oral Presentation) <b><font color="red">[CCF B]</font></b> <br>
   [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9859793">paper</a>]
   [<a href="https://arxiv.org/pdf/2110.05170">arxiv</a>]
   [<a href="https://ieeexplore.ieee.org/document/9859793">DOI</a>]
   [<a href="https://github.com/qianyuzqy/RCCR">code</a>]
   [<a href="bibs/ICME_2022_RCCR.txt">bib</a>] 
   </font>
   </p> </li>

    <li> <p style="margin-left: 0px; line-height: 150%; margin-top: 10px; margin-bottom: 10px;"><font face="Arial" size="3"><meta charset="utf-8">
            DMT: Dynamic Mutual Training for Semi-Supervised Learning <br> 
         <i>   Zhengyang Feng*, <b>Qianyu Zhou*</b>, Qiqi Gu, Xin Tan, Guangliang Cheng, Xuequan Lu, Jianping Shi, Lizhuang Ma. </i><br> 
         Patter Recognition (<b>PR</b>), vol. 130, no. C, pp. 108777-108789, 2022 <b><font color="red">[CCF B, JCR Q1]</font></b> <br>
   [<a href="https://www.sciencedirect.com/science/article/pii/S0031320322002588">paper</a>]
   [<a href="https://arxiv.org/pdf/2004.08514.pdf">arxiv</a>]
   [<a href="https://www.sciencedirect.com/science/article/pii/S0031320322002588">DOI</a>]
   [<a href="https://github.com/voldemortX/DST-CBC">code</a>]
   [<a href="bibs/PR_2022_DMT.txt">bib</a>]
   </font>
   </p> </li>


</ul>
<h3>2021:</h3> 
<ul>

    <li> <p style="margin-left: 0px; line-height: 150%; margin-top: 10px; margin-bottom: 10px;"><font face="Arial" size="3"><meta charset="utf-8">
            End-to-End Video Object Detection with Spatial-Temporal Transformers <br> 
         <i>   Lu He*, <b>Qianyu Zhou*</b>, Xiangtai Li*, Li Niu, Guangliang Cheng, Xiao Li, Wenxuan Liu, Yunhai Tong, Lizhuang Ma, Liqing Zhang. </i><br> 
         The 29th ACM International Conference on Multimedia (<b>ACM MM</b>), pp. 1507-1516, October, 2021 <b><font color="red">[CCF A]</font></b> <br>
   [<a href="https://dl.acm.org/doi/pdf/10.1145/3474085.3475285">paper</a>]
   [<a href="https://arxiv.org/pdf/2105.10920.pdf">arxiv</a>]
   [<a href="https://dl.acm.org/doi/10.1145/3474085.3475285">DOI</a>]
   [<a href="https://github.com/SJTU-LuHe/TransVOD">code</a>] 
   [<a href="bibs/MM_2021_TransVOD.txt">bib</a>]
   </font>
   </p> </li>

    <li> <p style="margin-left: 0px; line-height: 150%; margin-top: 10px; margin-bottom: 10px;"><font face="Arial" size="3"><meta charset="utf-8">
            PIT: Position-Invariant Transform for Cross-FoV Domain Adaptation <br> 
         <i>   Qiqi Gu*, <b>Qianyu Zhou*</b>, Minghao Xu, Zhengyang Feng, Guangliang Cheng, Xuequan Lu, Jianping Shi, Lizhuang Ma. </i><br> 
         IEEE/CVF International Conference on Computer Vision (<b>ICCV</b>), pp. 8741-8750, October, 2021 <b><font color="red">[CCF A]</font></b> <br>
   [<a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Gu_PIT_Position-Invariant_Transform_for_Cross-FoV_Domain_Adaptation_ICCV_2021_paper.pdf">paper</a>]
   [<a href="https://arxiv.org/pdf/2108.07142.pdf">arxiv</a>]
   [<a href="https://ieeexplore.ieee.org/document/9711500">DOI</a>] 
   [<a href="https://github.com/sheepooo/PIT-Position-Invariant-Transform">code</a>]
   [<a href="bibs/ICCV_2021_PIT.txt">bib</a>]
   </font>
   </p> </li>

    <li> <p style="margin-left: 0px; line-height: 150%; margin-top: 10px; margin-bottom: 10px;"><font face="Arial" size="3"><meta charset="utf-8">
            Label-free Regional Consistency for Image-to-Image Translation <br> 
         <i>   Shaohua Guo, <b>Qianyu Zhou<sup>†</sup></b>, Zhou Ye, Qiqi Gu, Junshu Tang, Zhengyang Feng, Lizhuang Ma<sup>†</sup>. </i><br> 
         IEEE International Conference on Multimedia and Expo (<b>ICME</b>), July, 2021  <b><font color="red">[CCF B]</font></b><br>
   [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9428211">paper</a>]
   [<a href="https://ieeexplore.ieee.org/document/9428211">DOI</a>]
   [<a href="bibs/ICME_2021_LFRC.txt">bib</a>]
   </font>
   </p> </li>
   
    <li> <p style="margin-left: 0px; line-height: 150%; margin-top: 10px; margin-bottom: 10px;"><font face="Arial" size="3"><meta charset="utf-8">
            Semi-Supervised 3D Object Detection Via Adaptive Pseudo-Labeling <br> 
         <i>  Hongyi Xu, Fengqi Liu, <b>Qianyu Zhou<sup>†</sup></b>, Jinkun Hao, Zhijie Cao, Zhengyang Feng, Lizhuang Ma<sup>†</sup>. </i><br> 
         IEEE International Conference on Image Processing (<b>ICIP</b>), September, 2021 <b><font color="red">[CCF C, TH-CPL B]</font></b> <br>
   [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9506421">paper</a>]
   [<a href="https://arxiv.org/pdf/2108.06649.pdf">arxiv</a>]
   [<a href="https://doi.org/10.1109/ICIP42928.2021.9506421">DOI</a>]
   [<a href="bibs/ICIP_2021_SS3D.txt">bib</a>]
   </font>

</ul>
</script>

<script>showPubs(0);</script>
<script>var scroll = new SmoothScroll('a[href*="#"]', {speed: 1000});</script>


<!-- <br>
<h2><font face="Arial"> Education </font></h2>
<div id="education">
<ul>
    <li> <p style="margin-left: 0px; line-height: 150%; margin-top: 10px; margin-bottom: 10px;"><font face="Arial" size="3"><meta charset="utf-8">
            Ph.D, <a href="https://www.cs.sjtu.edu.cn/en/" target="_blank" style="color: black;"> Department of Computer Science and Engineering</a>, <a href="https://en.sjtu.edu.cn/" target="_blank" style="color: black;">Shanghai Jiao Tong University</a>. (Sep. 2019 - Jun. 2024)<br>
            Memeber of <a href="https://xsb.seiee.sjtu.edu.cn/xsb/info/35105.htm" target="_blank" style="color: black;"> Wenjun Wu AI Honored Ph.D Class</a> (Top 1%), <a href="https://ai.sjtu.edu.cn/" target="_blank" style="color: black;"> AI Institute</a>, <a href="https://en.sjtu.edu.cn/" target="_blank" style="color: black;">Shanghai Jiao Tong University</a>; <br>
            Memeber of <a href="https://www.gs.sjtu.edu.cn/post/detail/Z3MxNDc2" target="_blank" style="color: black;"> Zhiyuan Honored Ph.D Program</a> (The Highest Honor) in <a href="https://en.sjtu.edu.cn/" target="_blank" style="color: black;">Shanghai Jiao Tong University</a>; <br>
            Advised by <a href="https://scholar.google.com.hk/citations?user=yd58y_0AAAAJ&hl=en" target="_blank" style="color: black;">Prof. Lizhuang Ma</a> and Dr. <a href="https://shijianping.me/" target="_blank" style="color: black;"> Jianping Shi</a> <br>
   </font>
     </p> </li>

    <li> <p style="margin-left: 0px; line-height: 150%; margin-top: 10px; margin-bottom: 10px;"><font face="Arial" size="3"><meta charset="utf-8">
            B.Sc, <a href="http://ccst.jlu.edu.cn/" target="_blank" style="color: black;">College of Computer Science and Technology</a>, <a href="http://global.jlu.edu.cn/" target="_blank" style="color: black;">Jilin University</a>. (Aug. 2015 - Jun. 2019)<br>
            Top Ten Honorary Undergraduates (The Highest Honor) in Jilin University;<br> 
   </font>
     </p> </li>


</ul>
</div> -->


<!--<h2><font face="Arial"> Experience </font></h2>
<div id="experience">
<ul>
    <li> <p style="margin-left: 0px; line-height: 150%; margin-top: 10px; margin-bottom: 10px;"><font face="Arial" size="3"><meta charset="utf-8">
            Research Intern at <a href="https://www.catl.com/en/">Contemporary Amperex Technology Co., Ltd (CATL)</a>, Ningde, P.R.China. (May. 2023 - Aug. 2023)<br> 
         Worked on surface defect detection, quality inspection for industrial manufacturing.<br> 
         Collaborators: <a href="https://hujiecpp.github.io/">Jie Hu</a> and <a href="https://scholar.google.com.au/citations?user=yw-rcj4AAAAJ&hl=en">Guannan Jiang</a> <br>
   </font>
     </p> </li>

    <li> <p style="margin-left: 0px; line-height: 150%; margin-top: 10px; margin-bottom: 10px;"><font face="Arial" size="3"><meta charset="utf-8">
            Research Intern at <a href="https://open.youtu.qq.com/#/open">Youtu Lab</a>, <a href="https://www.tencent.com/en-us/index.html">Tencent</a>, Shanghai, P.R.China. (July. 2021 - Nov. 2022)<br> 
         Worked on face anti-spoofing, biometrics security.<br> 
         Mentor: <a href="https://sndler.github.io/">Taiping Yao</a> and <a href="https://scholar.google.com.hk/citations?user=i2ah3-wAAAAJ&hl=en">Ke-Yue Zhang</a> <br>
   </font>
     </p> </li>

    <li> <p style="margin-left: 0px; line-height: 150%; margin-top: 10px; margin-bottom: 10px;"><font face="Arial" size="3"><meta charset="utf-8">
            Research Intern at <a href="https://www.sensetime.com/en">Sensetime Reserch</a>, Beijing & Shanghai, P.R.China. (July. 2019 - Mar. 2021)<br> 
         Worked on scene understanding, autonomous driving. <br> 
         Mentor: <a href="https://scholar.google.com.hk/citations?user=FToOC-wAAAAJ&hl=en">Guangliang Cheng</a> and <a href="https://scholar.google.com.hk/citations?user=mwsxrm4AAAAJ&hl=en">Jianping Shi</a> <br>
   </font>
     </p> </li>

</ul>
</div>
 -->



<!-- <h2><font face="Arial"> Experience </h2>
<ul style="list-style-type:none">
<p style="line-height: 120%; margin-left: 0px; margin-top: 8pt; margin-bottom: 8pt;"> <font face="Arial" size="3">
<li>Research Intern, <a href="https://www.catl.com/en/">Contemporary Amperex Technology Co., Ltd </a>  (Mentor: <a href="https://hujiecpp.github.io/">Jie Hu</a> and <a href="https://scholar.google.com.au/citations?user=yw-rcj4AAAAJ&hl=en">Guannan Jiang</a>) <div style="float:right; text-align:right">May. 2023 - Aug. 2023</div><br>
<p style="line-height: 120%; margin-left: 0px; margin-top: 8pt; margin-bottom: 8pt;"> <font face="Arial" size="3">
<li>Remote Intern, <a href="http://rongcheer.com/enproducts.asp">Rongcheer Industrial Technology Co., Ltd </a> (Manager: Yan Tang and <a href="http://rongcheer.com/NewsShow.asp?Bid=32">Wenbing Zhu</a>) <div style="float:right; text-align:right">April. 2023-May. 2023</div><br>
<li>Research Intern, <a href="https://open.youtu.qq.com/#/open">Youtu Lab</a>, <a href="https://www.tencent.com/en-us/index.html">Tencent</a> (Mentor: <a href="https://sndler.github.io/">Taiping Yao</a> and <a href="https://scholar.google.com.hk/citations?user=i2ah3-wAAAAJ&hl=en">Ke-Yue Zhang</a>) <div style="float:right; text-align:right">July. 2021 - Nov. 2022</div><br>
<li>Research Intern, <a href="https://www.sensetime.com/en">Sensetime Reserch</a> (Mentor: <a href="https://scholar.google.com.hk/citations?user=FToOC-wAAAAJ&hl=en">Guangliang Cheng</a> and <a href="https://scholar.google.com.hk/citations?user=mwsxrm4AAAAJ&hl=en">Jianping Shi</a>)<div style="float:right; text-align:right">July. 2019 - Mar. 2021</div><br>
</p>
</ul> -->

<h2><font face="Arial"> Academic Services </font></h2>
<div id="service">
<!-- <a name="Services" /> -->
<ul style="list-style-type:none"> 
     <li> <p style="margin-left: 0px; line-height: 150%; margin-top: 8px; margin-bottom: 8px;"><font face="Arial" size="3"><meta charset="utf-8">
             <strong> Oral Session Chair </strong> <br> </font> </p> 
        <li> <p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;">
            ACM International Conference on Multimedia (ACM MM), 2024 </p></li>

     <li> <p style="margin-left: 0px; line-height: 150%; margin-top: 8px; margin-bottom: 8px;"><font face="Arial" size="3"><meta charset="utf-8">
         <strong> Journal Reviewer </strong> <br> </font> </p> 
    <li> <p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px; color: black">
        <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34" style="color: black;">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) </a> </p></li>
    <li> <p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;">
         <a href="https://www.springer.com/journal/11263" style="color: black;">International Journal of Computer Vision (IJCV)</a> </p></li>
    <li> <p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;">
        <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=10206" style="color: black;"> IEEE Transactions on Information Forensics & Security (TIFS) </a> </p>
    <li> <p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;">
        <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6046" style="color: black;"> IEEE Transactions on Multimedia (TMM) </a> </p>
    <li> <p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;">
        <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=76" style="color: black;"> IEEE Transactions on Circuits and Systems for Video Technology (TCSVT) </a> </p>
    <li> <p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;">
        <a href="https://www.sciencedirect.com/journal/information-fusion" style="color: black;">Information Fusion (INFFUS) </a> </p>
    <li> <p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;">
        <a href="https://www.sciencedirect.com/journal/pattern-recognition" style="color: black;">Pattern Recognition (PR) </a> </p>
    <li> <p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;">
        <a href="https://www.sciencedirect.com/journal/neurocomputing/" style="color: black;">Neurocomputing </a> </p>
    </li>
     <li> <p style="margin-left: 0px; line-height: 150%; margin-top: 8px; margin-bottom: 8px;"><font face="Arial" size="3"><meta charset="utf-8">
         <strong> Conference Reviewer </strong> <br> </font> </p> 
    <li> <p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;">
        IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022 - 2025 </p></li>
    <li> <p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;">
        IEEE International Conference on Computer Vision (ICCV), 2023, 2025 </p></li>
    <li> <p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;">
        European Conference on Computer Vision (ECCV), 2022, 2024 </p></li>
    <li> <p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;">
        Annual Conference on Neural Information Processing Systems (NeurIPS), 2024 - 2025 </p></li>
    <li> <p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;">
        International Conference on Machine Learning (ICML), 2024 </p></li>
    <li> <p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;">
        International Conference on Learning Representations (ICLR), 2025 </p></li>
    <li> <p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;">
        AAAI Conference on Artificial Intelligence (AAAI), 2023 - 2025 </p></li>
    <li> <p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;">
        ACM International Conference on Multimedia (ACM MM), 2023 - 2025 </p></li>
    <li> <p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;">
        IEEE International Conference on Multimedia and Expo (ICME), 2023 - 2024 </p></li>
</ul>
</div>


<h2><font face="Arial"> Honors &amp; Awards </h2>
<div id="honors">
<ul style="list-style-type:none">
<p style="line-height: 120%; margin-left: 0px; margin-top: 8pt; margin-bottom: 8pt;"> <font face="Arial" size="3">
<li>World Internet Conference (WIC) award for Pioneering Science and Technology, 2024 <div style="float:right; text-align:right">2024</div><br>
<li>Zhiyuan Honored Ph.D Fellowship, Shanghai Jiao Tong University <div style="float:right; text-align:right">2019 - 2024</div><br>
<li>Wenjun wu AI Honored Ph.D Fellowship, Shanghai Jiao Tong University <div style="float:right; text-align:right">2019 - 2024</div><br>
<li>Yuanqing Yang Ph.D Fellowship, Shanghai Jiao Tong University <div style="float:right; text-align:right">2023</div><br>
<li>Best Paper Award in CAD/Graphics 2023 <div style="float:right; text-align:right">2023</div><br>  
<li>Top 3% Paper Recognition of all papers accepted at ICASSP 2023 <div style="float:right; text-align:right">2023</div><br>
<li>First Class Scholarship of DMCV Lab, Shanghai Jiao Tong University <div style="float:right; text-align:right">2022</div><br>
<li>National Scholarship, Ministry of Education of P.R. China<div style="float:right; text-align:right">2016 - 2018</div><br>
<li>President Scholarship of Jilin University<div style="float:right; text-align:right">2019</div><br>
<li>Outstanding Graduates of Jilin University<div style="float:right; text-align:right">2019</div><br>
<li>Top Ten Honorary Undergraduates (The Highest Honor), Jilin University <div style="float:right; text-align:right">2019</div><br>
</p>
</ul>
</div>



<!-- <h2><font face="Arial"> Students/Coauthorship </font></h2>
<div id="students">
    <ul>
        <li>
            <p style="margin-left: 0px; line-height: 150%; margin-top: 10px; margin-bottom: 10px;"><font face="Arial" size="3"><meta charset="utf-8">Ph.D Students</font></p>
            <ul>
                <li><p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;">Hao Yang, Shanghai Jiao Tong University, Co-supervise with Prof. Lizhuang Ma</p></li>
                <li><p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;"><a href="https://www.jincenjiang.com/" style="color: black;">Jincen Jiang</a>, Bournemouth University, Co-supervise with Prof. Xuequan Lu</p></li>
                <li><p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;"><a href="https://www.linkedin.com/in/yinghui-li-john/" style="color: black;">Yinghui Li</a>, Deakin University, Co-supervise with Prof. Xuequan Lu</p></li>
                <li><p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;">Zitong Wang, Northeast Normal University -> Jilin University</p></li>
            </ul>
        </li>
        <li>
            <p style="margin-left: 0px; line-height: 150%; margin-top: 10px; margin-bottom: 10px;"><font face="Arial" size="3"><meta charset="utf-8">Alumni</font></p>
            <ul>
                <li><p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;"><a href="https://scholar.google.com.hk/citations?user=tus0s1kAAAAJ&hl=en&oi=ao" style="color: black;">Yiran Song</a>, Shanghai Jiao Tong University (Currently at University of Minnesota), Co-supervised with Lizhuang Ma</p></li>
                <li><p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;"><a href="https://scholar.google.com.hk/citations?user=Tplq8DgAAAAJ&hl=en&oi=ao" style="color: black;">Qiqi Gu</a>, Shanghai Jiao Tong University (Currently at Tencent), Co-supervised with Prof. Lizhuang Ma</p></li>
                <li><p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;">Shaocong Long, Shanghai Jiao Tong University, Co-supervise with Prof. Yuan Luo</p></li>
                <li><p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;"><a href="https://scholar.google.com.hk/citations?user=WFoZVjEAAAAJ&hl=en&oi=ao" style="color: black;">Zhengyang Feng</a>, Shanghai Jiao Tong University (Currently at Momenta), Co-supervised with Prof. Lizhuang Ma</p></li>
            </ul>
        </li>
    </ul>
</div>

 -->

<br>
 <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=x6jtKeC44vFualcc13VnRDgl0HH2yt7AUpWIYw2ox54&cl=ffffff&w=300&t=tt"></script>  
 <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.4/jquery.min.js" integrity="sha3848gBf6Y4YYq7Jx97PIqmTwLPin4hxIzQw5aDmUg/DDhul9fFpbbLcLh3nTIIDJKhx" crossorigin="anonymous"></script>
<script src="script.js"></script><table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> 
<div id="footer">
    <div id="footer-text"></div>
</div>
    <p></p><center>
        <br>
            © Qianyu Zhou | Last updated: 12/2024
        </center><p></p>

</b></b></b></div><b><b><b>

</b></b></b></body>
<!-- {% include analytics.html %}
 -->
</html>
